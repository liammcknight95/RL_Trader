{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import LOBData\n",
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import gzip\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "from func_tools import normalize, get_labels, cnn_data_reshaping, reshape_lob_levels, plot_labels, label_insights, get_pnl\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing inputs\n",
    "security = 'USDT_BTC'\n",
    "raw_data_path = f'S3_data' # where json data is stored\n",
    "root_caching_folder = \"Processed_Data\"\n",
    "frequency = timedelta(seconds=10)\n",
    "norm_type = 'dyn_z_score'\n",
    "\n",
    "# labelling inputs\n",
    "k_plus = 30#60\n",
    "k_minus = 30#60\n",
    "alpha = 0.001#0.0005\n",
    "roll = 7200 * 6 # step from minute to 10 second data\n",
    "# pull data from S3\n",
    "#download_s3_data('limit-order-books-data-po-limitorderbooksnapshots-v25ungbmmak9', pair)\n",
    "\n",
    "# Data import - needs to be adjusted importing from several files using Dask\n",
    "data = pd.read_csv(f'{root_caching_folder}/{security}/data-cache-10s.csv', index_col=0)\n",
    "lob_depth = data['Level'].max() + 1 # number of levels of order book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_test_split = int((data.shape[0] / lob_depth) * 0.7) # slice reference for train and test\n",
    "train_timestamps = data['Datetime'].unique()[:train_test_split]\n",
    "test_timestamps = data['Datetime'].unique()[train_test_split:]\n",
    "\n",
    "train_cached_data = data[data['Datetime'].isin(train_timestamps)].set_index(['Datetime', 'Level'])\n",
    "test_cached_data = data[data['Datetime'].isin(test_timestamps)].set_index(['Datetime', 'Level'])\n",
    "\n",
    "print(f'Train dataset shape: {train_cached_data.shape} - Test dataset shape: {test_cached_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # z-score formula\n",
    "# mean_rw = np.mean(stacked_series.iloc[0:roll * 10 * 2])\n",
    "# std_rw = np.std(stacked_series.iloc[0:roll * 10 * 2])\n",
    "# new_data = stacked_series[(roll * 10 * 2):(roll * 20 * 2) + (10 * 2)]\n",
    "# z_rw = (new_data - mean_rw) / std_rw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataNormalization:\n",
    "\n",
    "    def __init__(self, ts, roll, ob_levels, start=0):\n",
    "        ''' \n",
    "            ts: pd.Series or pd.Dataframe. If dataframe, need to have cols that can be normalized\n",
    "                together, like all prices or sizes\n",
    "\n",
    "            roll: int, rolling window (depends on frequency of data passed)\n",
    "\n",
    "            ob_levels: int, orderbook depth. Assumed to be constant throughtout all timeseries\n",
    "            \n",
    "            start: int, at which point of the timeseries the rolling start. Has to be a multiple of \n",
    "                    ob_levels * n df columns\n",
    "        '''\n",
    "        self.ts = ts\n",
    "        self.roll = roll\n",
    "        self.ob_levels = ob_levels\n",
    "        self.ts_shape = self.ts.shape[1]\n",
    "        self.roll_window = self.roll * self.ob_levels * self.ts_shape\n",
    "        self.roll_step = self.ob_levels * self.ts_shape\n",
    "        self.start = start\n",
    "        self.ts_stacked = self.get_ts_stack() # stack dataframe as default\n",
    "        self.new_data = pd.Series()\n",
    "        self.dyn_ts = pd.Series()\n",
    "    \n",
    "    def get_ts_stack(self):\n",
    "        ''' Flatten dataframe into a series if more than 1 column is passed '''\n",
    "        if self.ts_shape > 1:\n",
    "            self.ts_stacked = self.ts.stack()\n",
    "            #print(self.ts_stacked)\n",
    "        else:\n",
    "            self.ts_stacked = self.ts\n",
    "        return self.ts_stacked\n",
    "\n",
    "    def get_new_data(self):\n",
    "        ''' Add 1 roll step to the self.start variable, to get next timestep from dataframe '''\n",
    "        self.start += self.roll_step\n",
    "        self.new_data = self.ts_stacked.iloc[(self.start+self.roll_window):(self.start+self.roll_window+self.roll_step)]\n",
    "        return self.new_data\n",
    "\n",
    "    def get_one_dyn_z(self):\n",
    "        ''' Calculate 1 period dynamic z score - 1/100th of a second'''\n",
    "        mean_rw = np.mean(self.ts_stacked.iloc[self.start:self.roll_window+self.start])\n",
    "        std_rw = np.std(self.ts_stacked.iloc[self.start:self.roll_window+self.start])\n",
    "        # self.start is updated in get_new_data, so get_new_data() has to be executed after mean_rw and std_rw\n",
    "        self.new_data = self.get_new_data() \n",
    "        #print(self.ts_stacked.iloc[self.start:self.roll_window+self.start])\n",
    "        #print(self.new_data)\n",
    "        z_rw = (self.new_data - mean_rw) / std_rw\n",
    "        return z_rw\n",
    "        \n",
    "    def get_ts_dyn_z(self):\n",
    "        ''' Loop through all time series - much slower than pandas rolling implementation '''\n",
    "        while self.roll_window+self.start <= self.ts_stacked.shape[0]:\n",
    "            self.dyn_ts = pd.concat([self.dyn_ts, self.get_one_dyn_z()])\n",
    "        return self.dyn_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_class = DataNormalization(test_cached_data[['Ask_Price', 'Bid_Price']], roll, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = norm_class.get_ts_dyn_z()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_class.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_class.ts_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_class.roll_window + norm_class.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cached_data.iloc[[1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dyn_df,dyn_df2,pd.Series()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "norm_class.get_one_dyn_z()\n",
    "dyn_df2 = norm_class.get_one_dyn_z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_class = DataNormalization(test_cached_data[['Ask_Size', 'Bid_Size']], roll, 10, 0)\n",
    "dyn_df = norm_class.dyn_z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn_df.reset_index().pivot_table(index=['Datetime', 'Level'], columns='level_2', values=0, dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dyn_df = pd.read_csv(f'{root_caching_folder}/{security}/TEST-{lob_depth}-{norm_type}-{roll}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dyn_df[test_dyn_df['Datetime']=='2020-08-14 19:27:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_series.iloc[0:roll * 10 * 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(test_cached_data[['Ask_Price', 'Bid_Price']], lob_depth, 'dyn_z_score', roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessing = LOBData.LOBData(raw_data_path, security, root_caching_folder, frequency=timedelta(seconds=10), levels=100, resampled_cache='1min')\n",
    "raw_data = preprocessing.get_LOB_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f'{root_caching_folder}/{security}/2020-11-12-original_frequency.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Datetime'] = pd.to_datetime(df1['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by=['Sequence', 'Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df1[df1['Level']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_0['Datetime'] - df_0['Datetime'].shift()).dt.total_seconds()[:50]#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Level']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1['Datetime'] - df1['Datetime'].shift()).dt.total_seconds().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1['Datetime'] - df1['Datetime'].shift()).dt.total_seconds().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1['Datetime'] - df1['Datetime'].shift()).dt.total_seconds().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'{root_caching_folder}/{security}/test-data-cache_1min_2020-11-11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(8571500) / 144000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(f'{raw_data_path}/{security}/2020/11/11/20201111_00-0-0.json.gz', 'r') as f:\n",
    "    json_string = f.read().decode('utf-8')\n",
    "    frozen = json_string.count('\"isFrozen\": \"1\"')\n",
    "    if frozen > 0:\n",
    "        print(f'Frozen {frozen} snapshots')\n",
    "raw_data = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.update(raw_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_file['BTC_ETH-20201111_000956']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "# TODO - datetime as keys to sort later\n",
    "for key in raw_data.keys():\n",
    "    # unravel the nested json structure into a more manageable list of lists\n",
    "    [processed_data.append(list(zip(\n",
    "        [i[0[0:3000]] for i in raw_data.get(key)['asks'][0:levels]], # ask px\n",
    "        [i[1] for i in raw_data.get(key)['asks'][0:levels]], # ask size\n",
    "        [i[0] for i in raw_data.get(key)['bids'][0:levels]], # bid px\n",
    "        [i[1] for i in raw_data.get(key)['bids'][0:levels]], # bid size\n",
    "        list(range(levels)), # ob level - assuming same for both\n",
    "        [raw_data.get(key)['seq']] * levels,\n",
    "        [key[-15:]] * levels  # datetime part of the key\n",
    "    )))]\n",
    "# TODO sort datetime keys and cache one day as csv?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(raw_data.keys(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[processed_data, raw_data.get(key)['seq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unravel nested structure and force data types\n",
    "df = pd.DataFrame([y for x in processed_data for y in x], #flatten the list of lists structure\n",
    "                columns = ['Ask_Price', 'Ask_Size', 'Bid_Price', 'Bid_Size','Level', 'Sequence','Datetime'])\n",
    "\n",
    "df['Ask_Price'] = df['Ask_Price'].astype('float64')\n",
    "df['Ask_Size'] = df['Ask_Size'].astype('float64')\n",
    "df['Bid_Price'] = df['Bid_Price'].astype('float64')\n",
    "df['Bid_Size'] = df['Bid_Size'].astype('float64')\n",
    "df['Level'] = df['Level'].astype('int64')\n",
    "df['Sequence'] = df['Sequence'].astype('int64')\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'], format='%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(processed_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "preprocessing = LOBData.LOBData(raw_data_path, security, root_caching_folder, frequency=timedelta(seconds=10), levels=100, resampled_cache='1min')\n",
    "raw_data = preprocessing.get_LOB_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f'{root_caching_folder}/{security}/2020-11-11 00:00:00-original_frequency.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_csv(f'{root_caching_folder}/{security}/2020-11-12 00:00:00-original_frequency.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Level']==0].head(5000)['Bid_Price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Level']==0].head(5000)['Bid_Price'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 'BTC_ETH/20201112_14-0-3.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day.split(\".\")[0].split(\"/\")[1].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(f'{root_caching_folder}/{security}/original_frequency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.sort_values(by='Sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = pd.to_datetime(raw_data['Datetime'].unique()) - pd.to_datetime(pd.Series(raw_data['Datetime'].unique()).shift(periods=1))\n",
    "print(deltas.describe()), print('######'), deltas.sort_values(ascending=False).tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_anomalyes = pd.concat([pd.Series(raw_data['Datetime'].unique()), deltas], axis=1)\n",
    "df_time_anomalyes[df_time_anomalyes.index.isin(deltas[deltas != pd.Timedelta(seconds=10)].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(deltas, log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_cache.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(minute_cache[minute_cache.Level == 0], y='Ask_Size', x='Datetime')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_cache = pd.read_csv(f'{root_caching_folder}/{security}/test-data-cache-1m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_minute = pd.to_datetime(minute_cache['Datetime'].unique()) - pd.to_datetime(pd.Series(minute_cache['Datetime'].unique()).shift(periods=1))\n",
    "print(deltas_minute.describe()), print('######'), deltas_minute.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "source": [
    "## Heavy lifting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import func_tools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # initiate values to print out under dash components\n",
    "    k_plus_window_text = ''\n",
    "    k_minus_window_text = ''\n",
    "    alpha_thresh_text = ''\n",
    "    tr_fee_text = ''\n",
    "    \n",
    "    # data reading\n",
    "    data = pd.read_csv(f'{root_caching_folder}/{security}/data-cache-1m.csv', index_col=0)\n",
    "    data = data[(data.Datetime >= start_date) & (data.Datetime <= end_date)]\n",
    "\n",
    "    data_top = data[data.Level  == 0]#.reset_index() #fix double index issue. Do it the func tool way, cause that's the one that changes\n",
    "    data_top['Mid_Price'] = (data_top['Ask_Price'] + data_top['Bid_Price']) / 2\n",
    "    data_top['Spread'] = (data_top['Ask_Price'] - data_top['Bid_Price']) / data_top['Mid_Price']\n",
    "    data_grouped = data_top.groupby('Datetime').agg(\n",
    "        {'Ask_Size':'sum',\n",
    "        'Bid_Size':'sum',\n",
    "        'Spread': 'min'\n",
    "        }\n",
    "    )\n",
    "    #bbo_df.index = bbo_df.index.set_names(['date'])\n",
    "    #bbo_df = bbo_df.reset_index()\n",
    "    #print(bbo_df.tail(10))\n",
    "    \n",
    "    #print(security)\n",
    "    \n",
    "    px_chart = make_subplots(rows=2, cols=1, subplot_titles=(\"\", \"10 levels depth and spread\"), shared_xaxes=True,\n",
    "                            row_heights=[0.6, 0.4], vertical_spacing = 0.10, \n",
    "                            specs=[[{\"secondary_y\": True}], [{\"secondary_y\": True}]])\n",
    "\n",
    "    pnl_chart = px.line(height=200)\n",
    "    #for i in range(len(norm_type)):\n",
    "    sec_axis_check = False\n",
    "\n",
    "    # add depth and spread to main chart\n",
    "    px_chart.add_trace(go.Scatter(x=data_grouped.index.values, y=data_grouped['Bid_Size'].values,  name='Bid depth - 10 levels',\n",
    "                                marker=dict(color='#81C342')), row=2, col=1, secondary_y=False) # fill down to xaxis\n",
    "\n",
    "    px_chart.add_trace(go.Scatter(x=data_grouped.index.values, y=-data_grouped['Ask_Size'].values,  name='Ask depth - 10 levels',\n",
    "                                marker=dict(color='#EB2030')), row=2, col=1, secondary_y=False) # fill down to xaxis\n",
    "\n",
    "    px_chart.add_trace(go.Scatter(x=data_grouped.index.values, y=data_grouped['Spread'].values,  name='Best bid-offer spread',\n",
    "                                marker=dict(color='#335eff')), row=2, col=1, secondary_y=True) # fill down to xaxis\n",
    "\n",
    "\n",
    "\n",
    "    px_chart.add_trace(go.Scatter(x=data_top['Datetime'], y=data_top['Mid_Price'], name='price', marker=dict(color='#000000')), \n",
    "                        row=1, col=1, secondary_y=False)\n",
    "    px_chart.update_yaxes(title_text=\"$ price\", secondary_y=sec_axis_check, row=1, col=1)\n",
    "    sec_axis_check = True\n",
    "\n",
    "\n",
    "\n",
    "    # if 'z_score' in norm_type:\n",
    "    #     norm_ts = ft.normalize(data[['Ask_Price', 'Bid_Price']], ob_levels=ob_levels, norm_type='z_score')\n",
    "    #     px_chart.add_trace(go.Scatter(x=data.index, y=norm_ts, name='z-score', marker=dict(color='#19D3F3')), \n",
    "    #                         row=1, col=1, secondary_y=sec_axis_check)\n",
    "    #     px_chart.update_yaxes(title_text=\"normalized price\", secondary_y=sec_axis_check, row=1, col=1)\n",
    "\n",
    "    #if 'dyn_z_score' in norm_type:\n",
    "    data_ft = data.set_index(['Datetime', 'Level'])\n",
    "    norm_ts_px = ft.normalize(data_ft[['Ask_Price', 'Bid_Price']], ob_levels=ob_levels, norm_type='dyn_z_score', roll=norm_window)\n",
    "    norm_ts_vol = ft.normalize(data_ft[['Ask_Size', 'Bid_Size']], ob_levels=ob_levels, norm_type='dyn_z_score', roll=norm_window) # get norm volumes\n",
    "    test_dyn_df = pd.concat([norm_ts_px, norm_ts_vol], axis=1).reset_index() # concat along row index\n",
    "    depth_dyn, dt_index_dyn = ft.reshape_lob_levels(test_dyn_df, output_type='array') # 1 train dataset\n",
    "    mid_px_train_dyn = pd.Series((depth_dyn[:,2] + depth_dyn[:,0]) / 2) # 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# specify number of levels as well as frequency in caches\n",
    "# add frequency as class parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow\n",
    "# for device in tensorflow.config.experimental.list_physical_devices('GPU'):\n",
    "#     tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, LeakyReLU, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import LOBData\n",
    "#from func_tools import normalize, get_labels, cnn_data_reshaping, reshape_lob_levels, plot_labels, label_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tensorflow.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tensorflow.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tensorflow.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tensorflow.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fun(x):\n",
    "#     x[0]=5\n",
    "#     return x\n",
    "# g = [10,11,12]\n",
    "\n",
    "# print(g)\n",
    "\n",
    "# f = fun(g)\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['Amir', 'Barry', 'Char', 'Delp']\n",
    "# print(names[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing inputs\n",
    "security = 'USDC_BTC'\n",
    "raw_data_path = f'Pawel_test' # where json data is stored\n",
    "root_caching_folder = \"Processed_Data\"\n",
    "frequency = timedelta(seconds=10)\n",
    "levels = 10\n",
    "\n",
    "\n",
    "# labelling inputs\n",
    "k_plus = 60\n",
    "k_minus = 60\n",
    "alpha = 0.0005\n",
    "roll = 7200\n",
    "# pull data from S3\n",
    "#download_s3_data('limit-order-books-data-po-limitorderbooksnapshots-v25ungbmmak9', pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "preprocess = True\n",
    "\n",
    "if preprocess == True:\n",
    "\n",
    "    preprocessing = LOBData.LOBData(raw_data_path, security, root_caching_folder, frequency=timedelta(seconds=10), levels=10)\n",
    "    raw_data = preprocessing.get_LOB_data()\n",
    "    raw_data.to_csv(f'Processed_Data/{security}/data-cache-10s-test.csv') # save raw data full depth 10 secs as csv, can be a big file\n",
    "    \n",
    "    data = pd.read_csv(f'{root_caching_folder}/{security}/data-cache-10s-test.csv', index_col=0)\n",
    "\n",
    "elif preprocess == False:\n",
    "    data = pd.read_csv(f'{root_caching_folder}/{security}/data-cache-10s-test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{root_caching_folder}/{security}/test-data-cache-1m.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Datetime'] = pd.to_datetime(data['Datetime'], format='%Y%m%d_%H%M%S')\n",
    "data['Datetime'] = pd.to_datetime(data['Datetime'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = data.groupby([pd.Grouper(key='Datetime', freq='1h'), pd.Grouper(key='Level')]).last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partitions = [group for group in resampled_data.groupby([resampled_data.Datetime.dt.year, resampled_data.Datetime.dt.month, resampled_data.Datetime.dt.day])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caching_folder = f'{root_caching_folder}/{security}'\n",
    "cache_file = f'{caching_folder}/test-data-cache-1h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in df_partitions:\n",
    "    partition_date = '-'.join([str(x) for x in group[0]])\n",
    "    group[1].to_csv(f'{cache_file}_{partition_date}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'-'.join([str(x) for x in group[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Mid_Price'] = (data['Ask_Price'] + data['Bid_Price']) / 2\n",
    "# data['Spread'] = (data['Ask_Price'] - data['Bid_Price']) / data['Mid_Price']\n",
    "# data_grouped = data.groupby('Datetime').agg({'Ask_Size':'sum',\n",
    "#                               'Bid_Size':'sum',\n",
    "#                               'Spread': ['min', 'max']\n",
    "#                              })\n",
    "# data_grouped.head()"
   ]
  },
  {
   "source": [
    "### Train - Test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lob_depth = data['Level'].max() + 1 # number of levels of order book\n",
    "train_test_split = int((data.shape[0] / lob_depth) * 0.7) # slice reference for train and test\n",
    "train_timestamps = data['Datetime'].unique()[:train_test_split]\n",
    "test_timestamps = data['Datetime'].unique()[train_test_split:]\n",
    "\n",
    "train_cached_data = data[data['Datetime'].isin(train_timestamps)].set_index(['Datetime', 'Level'])\n",
    "test_cached_data = data[data['Datetime'].isin(test_timestamps)].set_index(['Datetime', 'Level'])\n",
    "\n",
    "print(f'Train dataset shape: {train_cached_data.shape} - Test dataset shape: {test_cached_data.shape}')"
   ]
  },
  {
   "source": [
    "### Normalize & check how train and test distributions differ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use normalize to calculate standardized z score version of the train data\n",
    "# train_z_prices = normalize(train_cached_data[['Ask_Price', 'Bid_Price']], 10, norm_type='z_score', roll=0) # get norm prices\n",
    "# train_z_volumes = normalize(train_cached_data[['Ask_Size', 'Bid_Size']], 10, norm_type='z_score', roll=0) # get norm volumes\n",
    "# train_z_df = pd.concat([train_z_prices, train_z_volumes], axis=1).reset_index() # concat along row index\n",
    "\n",
    "\n",
    "# # use normalize to calculate standardized z score version of the test data\n",
    "# test_z_prices = normalize(test_cached_data[['Ask_Price', 'Bid_Price']], 10, norm_type='z_score', roll=0) # get norm prices\n",
    "# test_z_volumes = normalize(test_cached_data[['Ask_Size', 'Bid_Size']], 10, norm_type='z_score', roll=0) # get norm volumes\n",
    "# test_z_df = pd.concat([test_z_prices, test_z_volumes], axis=1).reset_index() # concat along row index\n",
    "\n",
    "\n",
    "# display(train_z_df.describe())\n",
    "# display(test_z_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use normalize to calculate standardized z score version of the train data\n",
    "train_dyn_prices = normalize(train_cached_data[['Ask_Price', 'Bid_Price']], ob_levels=levels, norm_type='dyn_z_score', roll=roll) # get norm prices\n",
    "train_dyn_volumes = normalize(train_cached_data[['Ask_Size', 'Bid_Size']], ob_levels=levels, norm_type='dyn_z_score', roll=roll) # get norm volumes\n",
    "train_dyn_df = pd.concat([train_dyn_prices, train_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "\n",
    "\n",
    "# use normalize to calculate standardized z score version of the test data\n",
    "test_dyn_prices = normalize(test_cached_data[['Ask_Price', 'Bid_Price']], ob_levels=levels, norm_type='dyn_z_score', roll=roll) # get norm prices\n",
    "test_dyn_volumes = normalize(test_cached_data[['Ask_Size', 'Bid_Size']], ob_levels=levels, norm_type='dyn_z_score', roll=roll) # get norm volumes\n",
    "\n",
    "test_dyn_df = pd.concat([test_dyn_prices, test_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "\n",
    "\n",
    "display(train_dyn_df.describe())\n",
    "display(test_dyn_df.describe())"
   ]
  },
  {
   "source": [
    "### Reshape and Label"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### z-score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reshape to a format suitable for deep lob like training\n",
    "# train_depth_z, train_dt_index_z = reshape_lob_levels(train_z_df, output_type='array')\n",
    "# test_dept_z, test_dt_index_z = reshape_lob_levels(test_z_df, output_type='array')\n",
    "\n",
    "# # generate labels from z score mid px. Get mid stacking train and test bbo\n",
    "# mid_px_series_z = (pd.Series(np.hstack([train_depth_z[:,2], test_dept_z[:,2]])) + pd.Series(np.hstack([train_depth_z[:,0], test_dept_z[:,0]])))/2\n",
    "\n",
    "# # Decide whether to get labels from mid px or from standardized data\n",
    "# labels_z = get_labels(mid_px_series_z, k_plus, k_minus, alpha, long_only=False)"
   ]
  },
  {
   "source": [
    "#### dynamic z-score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to a format suitable for deep lob like training\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df, output_type='array')\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df, output_type='array')\n",
    "\n",
    "# # generate labels from z score mid px. Get mid stacking train and test bbo\n",
    "# mid_px_series_dyn = (pd.Series(np.hstack([train_depth_dyn[:,2], test_dept_dyn[:,2]])) + pd.Series(np.hstack([train_depth_dyn[:,0], test_dept_dyn[:,0]])))/2\n",
    "\n",
    "# # Decide whether to get labels from mid px or from standardized data\n",
    "# labels_dyn = get_labels(mid_px_series_dyn, k_plus, k_minus, alpha, long_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels from z score mid px. Get mid stacking train and test bbo\n",
    "# train\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2)\n",
    "labels_dyn_train = get_labels(mid_px_train_dyn, k_plus, k_minus, alpha, long_only=False)\n",
    "# test\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2)\n",
    "labels_dyn_test = get_labels(mid_px_test_dyn, k_plus, k_minus, alpha, long_only=False)"
   ]
  },
  {
   "source": [
    "### Visualize data and labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Labels\n",
    "# print('Train Labels')\n",
    "# train_transact_z = label_insights(labels_z[:train_test_split])\n",
    "# print('\\nTest Labels')\n",
    "# test_transact_z = label_insights(labels_z[train_test_split:])\n",
    "# print(f'\\nLabels Train as pctg of total: {test_transact_z/(test_transact_z+train_transact_z)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "print('Train Labels')\n",
    "train_transact_dyn = label_insights(labels_dyn_train)\n",
    "print('\\nTest Labels')\n",
    "test_transact_dyn = label_insights(labels_dyn_test)\n",
    "print(f'\\nLabels Train as pctg of total: {test_transact_dyn/(test_transact_dyn+train_transact_dyn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries useful for plotting\n",
    "mid_px_df = data[data['Level']==0].reset_index()\n",
    "mid_px_df['Mid_Price'] = (mid_px_df['Ask_Price'] + mid_px_df['Bid_Price']) / 2\n",
    "\n",
    "# datetime_index = np.hstack([train_dyn_df[train_dyn_df['Level']==0]['Datetime'], test_dyn_df[test_dyn_df['Level']==0]['Datetime']])\n",
    "\n",
    "# indexed_labels = pd.Series(data=labels_dyn.values, index=pd.Index(datetime_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(start, end, y0=0):\n",
    "    # Plot Data\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=1,specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.update_layout(title='<b>Visual check: values and labels</b>', title_x=0.5)\n",
    "\n",
    "    # fig.add_trace(go.Scatter(y=mid_px_series_z.values[:train_test_split], name='mix_px_z_train'))\n",
    "\n",
    "    # fig.add_trace(go.Scatter(y=mid_px_series_z.values[train_test_split:], x=np.arange(train_test_split,mid_px_series_z.shape[0]), name='mix_px_z_test'))\n",
    "\n",
    "    # fig.add_trace(go.Scatter(y=mid_px_df['Mid_Price'].values[start:end+roll], x=mid_px_df['Datetime'][start:end+roll], name='mix_px'), secondary_y=True)\n",
    "\n",
    "    fig.add_trace(go.Scatter(y=mid_px_train_dyn.values[start:end], x=mid_px_train_dyn.index[start:end], name='mix_px_dyn_train'))   \n",
    "\n",
    "    fig.add_trace(go.Scatter(y=labels_dyn_train[start:end], name='labels_encoded'), secondary_y=True)\n",
    "\n",
    "    # fig.add_trace(go.Scatter(y=mid_px_series_dyn.values[:train_test_split-roll], x=datetime_index[:train_test_split-roll], name='mix_px_dyn_train'))\n",
    "    # fig.add_trace(go.Scatter(y=mid_px_series_dyn.values[train_test_split-roll:], x=datetime_index[train_test_split-roll:], name='mix_px_dyn_test'))\n",
    "    \n",
    "    #x=np.arange(train_test_split,mid_px_series_dyn.shape[0]\n",
    "    background_color = plot_labels(labels_dyn_train[start:end], y0)\n",
    "    #fig.data[0].update(xaxis='x1')\n",
    "    fig.update_layout(shapes=background_color)\n",
    "    fig.update_layout(width=1200, height=600) # plot labels background\n",
    "    \n",
    "    fig.update_layout(xaxis2= {'anchor': 'x','overlaying': 'x', 'side': 'top'},\n",
    "                  yaxis_domain=[0, 1]);\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(0,10000,y0=0)\n",
    "# labels concern: not clear how the labelling will capture sudden short lived drops. Data gaps? Also not clear how labels on mid px rather than norm mid would compare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame(mid_px_series, columns=['mid_px_z'])\n",
    "dist_df['split'] =  dist_df.reset_index()['index'].apply(lambda x: 'train' if x <= train_test_split else 'test')\n",
    "\n",
    "fig = px.histogram(dist_df, x='mid_px_z', color='split', nbins=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build sine waves with depth - 4,5 levels to see if the predictions are ok asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data to prepare for 1D convolution (add extra dimension back for 2D)\n",
    "\n",
    "# train_X, train_Y = cnn_data_reshaping(train_depth, labels_reshaped[:train_test_split], T, conv_type='2D')\n",
    "# test_X, test_Y = cnn_data_reshaping(test_depth, labels_reshaped[train_test_split:], T, conv_type='2D')"
   ]
  },
  {
   "source": [
    "### Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to try to see if the model is capturing relations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deeplob(T, NF, number_of_lstm):\n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "    \n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    \n",
    "    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    print(convsecond_output.shape)\n",
    "    # use the MC dropout here\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "\n",
    "    # build the last LSTM layer\n",
    "    conv_lstm = LSTM(number_of_lstm)(conv_reshape)\n",
    "\n",
    "    # build the output layer\n",
    "    out = Dense(3, activation='softmax')(conv_lstm)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# deeplob = create_deeplob(100, 40, 64)\n",
    "# deeplob.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_deeplob(T, NF, number_of_lstm):\n",
    "    \n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(64, (1, 10))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    \n",
    "    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    print(convsecond_output.shape)\n",
    "    # use the MC dropout here\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]) * int(convsecond_output.shape[3]),))(convsecond_output)\n",
    "\n",
    "        # build the last LSTM layer\n",
    "    #conv_lstm = LSTM(number_of_lstm)(conv_reshape)\n",
    "    #dense_l = Dense(100, activation='softmax')(conv_reshape)\n",
    "    # build the output layer\n",
    "    conv_reshape = Dropout(rate=0.2)(conv_reshape)\n",
    "    out = Dense(3, activation='softmax')(conv_reshape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "ligh_deeplob = create_light_deeplob(100, 40, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ligh_deeplob.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(len(generator)):\n",
    "# \tx, y = generator[i]\n",
    "# \tprint('%s => %s' % (x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_depth.reshape(train_depth.shape + (1,))\n",
    "\n",
    "# y = np_utils.to_categorical(np.array(labels_reshaped),3)[:train_test_split]\n",
    "\n",
    "# generator = TimeseriesGenerator(\n",
    "#     x,\n",
    "#     y[:train_test_split],\n",
    "#     100,\n",
    "#     batch_size=64,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "#history = deeplob.fit(generator, epochs=20, verbose=1)#, validation_data=(, test_cat_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator[0][0].shape #reshape\n",
    "# generator[0][0].shape # no reshape\n",
    "# generator2[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 reshape to a format suitable for training\n",
    "# 1 generate labels from z score mid px. Get mid stacking train and test bbo\n",
    "\n",
    "# 3 generate labels from dyn z score mid px. Get mid stacking train and test bbo\n",
    "\n",
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df, output_type='array') # 1\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2) # 2\n",
    "labels_dyn_train = get_labels(mid_px_train_dyn, k_plus, k_minus, alpha, long_only=False) # 3\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df, output_type='array') # 1\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2) # 2\n",
    "labels_dyn_test = get_labels(mid_px_test_dyn, k_plus, k_minus, alpha, long_only=False) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the scene for TensorBoard\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = \"training_deep_lob/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tensorflow.keras.callbacks.ModelCheckpoint(\"light_lob_inc_dro_norm.h5\",\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1,\n",
    "                                                 period=1) # every epoch\n",
    "\n",
    "# Create a callback for early stopping: when sees no progress on the validation set\n",
    "es_callback = tensorflow.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                                       restore_best_weights=True)\n",
    "\n",
    "# Create a callback for TensorBoard\n",
    "tb_callback = tensorflow.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "#deeplob.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# Create data generator\n",
    "#x = train_depth.reshape(train_depth.shape + (1,))\n",
    "#y = np_utils.to_categorical(np.array(labels_reshaped),3)[:train_test_split]\n",
    "categorical_labels = np_utils.to_categorical(np.array(labels_reshaped),3)\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth,\n",
    "    categorical_labels[:train_test_split],\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth,\n",
    "    categorical_labels[train_test_split:],\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "ligh_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=1,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_lob_loaded_mock = tensorflow.keras.models.load_model(\"model_mock_1.h5\")\n",
    "# predictions = deep_lob_loaded_mock.predict(generator_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights\n",
    "#model.load_weights(latest)\n",
    "\n",
    "deep_lob_loaded = tensorflow.keras.models.load_model(\"light_lob.h5\")\n",
    "\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth,\n",
    "    np_utils.to_categorical(np.array(labels_reshaped),3)[train_test_split:],\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = deep_lob_loaded.evaluate(generator_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = deep_lob_loaded.predict(generator_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.array([0,0,1,1,0,0,-1])\n",
    "array2 = np.array([0,0,-1,-1,0,1])\n",
    "array3 = np.array([1,1,0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_utils.to_categorical(array1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_utils.to_categorical(array2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_utils.to_categorical(array3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np_utils.to_categorical(array2,3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array2[np.argmax(np_utils.to_categorical(array2,3), axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(px_series, labels):\n",
    "    # Plot Data\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=1,specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    fig.update_layout(title='<b>Visual check: values and labels</b>', title_x=0.5)\n",
    "\n",
    "    # add px series\n",
    "    fig.add_trace(go.Scatter(y=px_series.values, name='mix_px_train'))\n",
    "\n",
    "    background_color = plot_labels(labels)\n",
    "\n",
    "    fig.update_layout(shapes=background_color, width=1200, height=600) # plot labels background\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse engineer how to_categorical have assigned labels and map them back\n",
    "print(np.hstack([np.where(labels==0)[0][0], np.where(labels==1)[0][0], np.where(labels==-1)[0][0]]))# first element\n",
    "\n",
    "def back_to_labels(x):\n",
    "\n",
    "    if x == 0:\n",
    "        return 0\n",
    "\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "\n",
    "    elif x == 2:\n",
    "        return -1\n",
    "\n",
    "map_labels = np.vectorize(back_to_labels)\n",
    "mapped_labels = map_labels(np.argmax(predictions,axis=1))"
   ]
  }
 ]
}