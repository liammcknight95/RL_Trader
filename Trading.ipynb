{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import data_preprocessing as dp\n",
    "import backtrader as bt\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = timedelta(seconds=60)\n",
    "pair = 'USDT_BTC'\n",
    "date_start = '2021-10-02'\n",
    "date_end = '2021-11-12'\n",
    "lob_depth = 100\n",
    "norm_type = 'dyn_z_score'\n",
    "roll = 1440*10 # 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data, df_data_stdz = dp.import_data(\n",
    "#     pair, \n",
    "#     date_start, \n",
    "#     date_end, \n",
    "#     frequency=frequency, \n",
    "#     depth=lob_depth, \n",
    "#     norm_type=norm_type, \n",
    "#     roll=roll, \n",
    "#     stdz_depth=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_trade = dp.get_trade_data(pair, date_start, date_end, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_px = dp.get_lob_data(pair, date_start, date_end, frequency, lob_depth)\n",
    "df_px = dd.read_csv(results_px, compression='gzip').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_px.plot(x='Datetime', y='Mid_Price', figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_px['Datetime'] = pd.to_datetime(df_px['Datetime'])\n",
    "\n",
    "# resample data to a less granular frequency\n",
    "df_data = df_px.set_index('Datetime').asfreq('30min')\n",
    "# df_data['volume'] = df_data['amount_buy'] + df_data['amount_sell']\n",
    "\n",
    "data_resampled = df_data.resample('30min', label='right').agg( # closing time of candlestick\n",
    "    {\n",
    "    'Mid_Price': ['last', 'first', np.max, np.min], \n",
    "    # 'volume': np.sum\n",
    "    }\n",
    ")\n",
    "\n",
    "data_resampled.columns = data_resampled.columns.get_level_values(1)\n",
    "\n",
    "data_resampled['close'] = data_resampled['last']\n",
    "data_resampled['open'] = data_resampled['first']\n",
    "data_resampled['high'] = data_resampled['amax']\n",
    "data_resampled['low'] = data_resampled['amin']\n",
    "# data_resampled['volume'] = data_resampled['sum']\n",
    "data_resampled.index.name = 'datetime'\n",
    "\n",
    "data_resampled\n",
    "# rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resampled['log_ret'] = (np.log(data_resampled['close']) - np.log(data_resampled['close'].shift(1)))\n",
    "data_resampled['roll_std'] = data_resampled['log_ret'].rolling(window=336).std() # 336 is the number of 30mins interval in week\n",
    "data_resampled['roll_std'].plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtrader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Strategies.GoldenCross import GoldenCross\n",
    "from Strategies.BuyHold import BuyHold\n",
    "\n",
    "# Create a cerebro entity\n",
    "cerebro = bt.Cerebro()\n",
    "\n",
    "# Add a strategy\n",
    "cerebro.addstrategy(GoldenCross)\n",
    "\n",
    "# Create a Data Feed\n",
    "data = bt.feeds.PandasData(dataname=data_resampled[:2000])\n",
    "\n",
    "# Add the Data Feed to Cerebro\n",
    "cerebro.adddata(data)\n",
    "\n",
    "cerebro.addwriter(bt.WriterFile, out='./Strategies/logging/golden_cross2.csv', csv=True)\n",
    "\n",
    "# Set our desired cash start\n",
    "cerebro.broker.setcash(200000.0)\n",
    "# Add a FixedSize sizer according to the stake\n",
    "# cerebro.addsizer(bt.sizers.PercentSizer, percents=10)\n",
    "# cerebro.broker.setcommission(commission=0.0007) \n",
    "\n",
    "# Print out the starting conditions\n",
    "print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "\n",
    "# Run over everything\n",
    "\n",
    "\n",
    "cerebro.run()\n",
    "\n",
    "plt.rcParams['figure.figsize']=[22, 16]\n",
    "cerebro.plot()\n",
    "# Print out the final result\n",
    "print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "\n",
    "# figure out what's wrong with stop losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_results = pd.read_csv('./Strategies/logging/golden_cross2.csv', header=1, index_col='Id').dropna(thresh=3)\n",
    "strategy_results['datetime'] = pd.to_datetime(strategy_results['datetime'])\n",
    "print(strategy_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_and_indic = pd.merge(data_resampled, strategy_results, left_index=True, right_on='datetime', how='outer')\n",
    "print(strategy_and_indic.columns)\n",
    "columns_to_keep = ['datetime', 'open_x', 'close_x', 'high_x', 'low_x', 'cash', 'value', 'buy', 'sell', 'pnlplus', 'pnlminus', 'sma', 'sma.1', 'crossover']\n",
    "strategy_and_indic[columns_to_keep].to_csv('./Strategies/logging/golden_cross_cl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def saveplots(cerebro, numfigs=1, iplot=True, start=None, end=None,\n",
    "#              width=16, height=9, dpi=300, tight=True, use=None, file_path = '', **kwargs):\n",
    "\n",
    "#         from backtrader import plot\n",
    "#         if cerebro.p.oldsync:\n",
    "#             plotter = plot.Plot_OldSync(**kwargs)\n",
    "#         else:\n",
    "#             plotter = plot.Plot(**kwargs)\n",
    "\n",
    "#         figs = []\n",
    "#         for stratlist in cerebro.runstrats:\n",
    "#             for si, strat in enumerate(stratlist):\n",
    "#                 rfig = plotter.plot(strat, figid=si * 100,\n",
    "#                                     numfigs=numfigs, iplot=iplot,\n",
    "#                                     start=start, end=end, use=use)\n",
    "#                 figs.append(rfig)\n",
    "\n",
    "#         for fig in figs:\n",
    "#             for f in fig:\n",
    "#                 f.savefig(file_path, bbox_inches='tight')\n",
    "#         # return figs\n",
    "\n",
    "# saveplots(cerebro, file_path = 'savefig.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Strategy Backtester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Roadmap\n",
    "# for each trade I need entry price, closing price, number of periods, time in the trade, min, max, volatility V \n",
    "# make execution assumptions: conservative: enter trade next open bar, exit trade next open bar V\n",
    "# add stops and trailing stops - V static stops, TODO: trailing\n",
    "# wrap strategy in a reusable class - V TODO: refinements and add trading metrics method\n",
    "# pull more data, a few pairs and recent data (3 pairs, most recent data) - pending when pawel back, local storage\n",
    "# add single strategy to bitstamp account with cctx V\n",
    "# backtest multiple strategies across multiple pairs, splitting between train and test set etc\n",
    "# deploy multiple strategies\n",
    "\n",
    "## adjusted to accomodate for long only strategy without stop losses V\n",
    "## add stop losses fixing any potential issue V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trade profitability TODO: profitability dot not perfecty alligned\n",
    "# 0 on stop loss period impacting returns\n",
    "# probably need a refactoring to accomodate for multiple trades in the same period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ta\n",
    "from ta.volatility import BollingerBands, AverageTrueRange\n",
    "from ta.trend import EMAIndicator\n",
    "import config\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from StratTest.engine import TradingStrategy\n",
    "import itertools\n",
    "import ccxt\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = timedelta(seconds=60)\n",
    "pair = 'USDT_BTC'\n",
    "date_start = '2020-11-11'\n",
    "date_end = '2021-04-03'\n",
    "lob_depth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_px = dp.get_lob_data(pair, date_start, date_end, frequency, lob_depth)\n",
    "df_px = dd.read_csv(results_px, compression='gzip').compute()\n",
    "\n",
    "# prep\n",
    "df_px['Datetime'] = pd.to_datetime(df_px['Datetime'])\n",
    "\n",
    "# resample data to a less granular frequency - \n",
    "df_px = df_px.set_index('Datetime').asfreq('1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = ('5min', '10min', '30min', '60min')\n",
    "stop_losses = (100, 300, 500, 600, 1000)\n",
    "mas_combinations = list(itertools.product(np.arange(5,105, 5), np.arange(10,260, 10), frequencies, stop_losses))\n",
    "# mas_combinations = [(combo[0], combo[1], combo[2], combo[3]) for combo in mas_combinations if combo[0]<combo[1]]\n",
    "# mas_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mas_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_strategy(short_ema, long_ema, freq, stop_loss):\n",
    "    \n",
    "    trading_strategy = TradingStrategy(df_px, frequency=freq)\n",
    "    trading_strategy.resample_data() # resampling\n",
    "\n",
    "\n",
    "    trading_strategy.add_strategy(\n",
    "        'EMACrossOverLO', \n",
    "        execution_type='current_bar_close',#'next_bar_open', 'current_bar_close, 'cheat_previous_close\n",
    "        stop_loss_bps=stop_loss,\n",
    "        comms_bps=50,\n",
    "        short_ema=short_ema,\n",
    "        long_ema=long_ema,\n",
    "        print_trades=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # strategy_performance.append()\n",
    "    return [short_ema, long_ema, freq, stop_loss, trading_strategy.trades_df['cum_trades_pctg_return'][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test different combinations of moving averages\n",
    "\n",
    "frequencies = ('5min', '10min', '30min', '60min')\n",
    "stop_losses = (100, 300, 500, 600, 1000)\n",
    "mas_combinations = list(itertools.product(np.arange(5,105, 5), np.arange(10,260, 10), frequencies, stop_losses))\n",
    "mas_combinations = [(combo[0], combo[1], combo[2], combo[3]) for combo in mas_combinations if combo[0]<combo[1]]\n",
    "\n",
    "strategy_performance = []\n",
    "\n",
    "for ma_combination, iteration in zip(mas_combinations, range(len(mas_combinations))):\n",
    "\n",
    "    if iteration % 100 == 0: print(f'Done the first {iteration} iterations')\n",
    "\n",
    "    short_ema = ma_combination[0]\n",
    "    long_ema = ma_combination[1]\n",
    "    freq = ma_combination[2]\n",
    "    stop_loss = ma_combinations[3]\n",
    "\n",
    "    result = test_strategy(short_ema, long_ema, freq, stop_loss)\n",
    "\n",
    "    strategy_performance.append(result)\n",
    "\n",
    "strategy_performance_df = pd.DataFrame(strategy_performance, columns=['short_ema', 'long_ema', 'frequency', 'stop_loss', 'performance']).sort_values('performance', ascending=False)\n",
    "strategy_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_results = []\n",
    "with mp.Pool(processes=8) as pool:\n",
    "    results = pool.starmap(test_strategy, mas_combinations)\n",
    "    test_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_performance_df = pd.DataFrame(results, columns=['short_ema', 'long_ema', 'frequency', 'stop_loss', 'performance']).sort_values('performance', ascending=False)\n",
    "strategy_performance_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_predict, TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import visualization_tools as viz_tools\n",
    "# regressor = RandomForestRegressor(n_esti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy = TradingStrategy(df_px, frequency='30min')\n",
    "trading_strategy.resample_data() # resampling\n",
    "\n",
    "df_rf_ = trading_strategy.df.copy()\n",
    "df_rf_['y'] = (np.log(df_rf_['close']) - np.log(df_rf_['close'].shift(1))).shift(-1).copy()\n",
    "df_rf_['y_cheat'] = (np.log(df_rf_['close']) - np.log(df_rf_['close'].shift(1))).shift(-1).copy()\n",
    "\n",
    "df_rf_['open_close_var'] = ((df_rf_['close'] - df_rf_['open']) / df_rf_['close']).rolling(48*7).mean()\n",
    "df_rf_['high_low_var'] = ((df_rf_['high'] - df_rf_['low']) / df_rf_['close']).rolling(48*7).mean()\n",
    "df_rf_['log_return_1'] = (np.log(df_rf_['close']) - np.log(df_rf_['close'].shift(1)))\n",
    "df_rf_['log_return_3'] = (np.log(df_rf_['close']) - np.log(df_rf_['close'].shift(1)))\n",
    "df_rf_['log_return_5'] = (np.log(df_rf_['close']) - np.log(df_rf_['close'].shift(1)))\n",
    "\n",
    "ema_indicator = EMAIndicator(df_rf_['close'], window=10)\n",
    "\n",
    "df_rf_['ema_10'] = ema_indicator.ema_indicator()\n",
    "\n",
    "ema_indicator = EMAIndicator(df_rf_['close'], window=20)\n",
    "\n",
    "df_rf_['ema_20'] = ema_indicator.ema_indicator()\n",
    "\n",
    "ema_indicator = EMAIndicator(df_rf_['close'], window=5)\n",
    "\n",
    "df_rf_['ema_5'] = ema_indicator.ema_indicator()\n",
    "\n",
    "\n",
    "df_rf_ = df_rf_.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['y_cheat','close', 'high', 'low', 'open', 'ema_10', 'ema_5', 'ema_20', 'log_return_1', 'log_return_3', 'open_close_var', 'high_low_var']\n",
    "\n",
    "X = df_rf_[features].copy()\n",
    "y = df_rf_[['y']].copy()\n",
    "\n",
    "# initial split to keep out part of the data for validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=False, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "test_size = 500\n",
    "max_train_size = X_train.shape[0] - test_size\n",
    "\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=5,\n",
    "    gap=48,\n",
    "    # max_train_size=5000,\n",
    "    test_size=test_size,\n",
    ")\n",
    "\n",
    "all_splits = list(ts_cv.split(X_train, y_train))\n",
    "\n",
    "for split in all_splits:\n",
    "\n",
    "    print(split[0][0], split[0][-1], split[1][0], split[1][-1])\n",
    "\n",
    "# Access individual splits\n",
    "# train_0, test_0 = all_splits[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.iloc[train_0] # X.iloc[test_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: decide whether a custom splitter to avoid leakage is needed\n",
    "# gap = 48 # 1 day in 30m bars\n",
    "# train_chunk_size = 1300+gap\n",
    "# test_chunk_size = 200\n",
    "\n",
    "\n",
    "# block_size = train_chunk_size + test_chunk_size\n",
    "\n",
    "# blocks = [X[n:n+block_size] for n in np.arange(0, X.shape[0], block_size)]\n",
    "\n",
    "# tr_test_blocks = [[block[gap:train_chunk_size], block[train_chunk_size:train_chunk_size+test_chunk_size]] for block in blocks]\n",
    "\n",
    "# validation = tr_test_blocks.pop()\n",
    "\n",
    "# len(tr_test_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"weather\",\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "]\n",
    "categories = [\n",
    "    [\"clear\", \"misty\", \"rain\"],\n",
    "    [\"spring\", \"summer\", \"fall\", \"winter\"],\n",
    "    [\"False\", \"True\"],\n",
    "    [\"False\", \"True\"],\n",
    "]\n",
    "ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "# numerical_features = features = ['close', 'high', 'low', 'open', 'ema_10', 'ema_5', 'ema_20', 'log_return_5', 'open_close_var', 'high_low_var']\n",
    "\n",
    "\n",
    "experimental_pipeline = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            #(\"categorical\", ordinal_encoder, categorical_columns),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    # HistGradientBoostingRegressor(\n",
    "    #     categorical_features=range(4),\n",
    "    # ),\n",
    "    RandomForestRegressor()\n",
    "\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    # 'scaler': [StandardScaler(), MinMaxScaler(), Normalizer(), MaxAbsScaler()],\n",
    "\t# 'selector__threshold': [0, 0.001, 0.01],\n",
    "\t'randomforestregressor__n_estimators': [10, 20], #50, 100, 200],\n",
    "\t'randomforestregressor__max_depth': [1, 3, 5], #7, 11, 15, 20, 40],\n",
    "\t'randomforestregressor__min_samples_leaf': [1, 2, 4, 8, 16]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    experimental_pipeline, \n",
    "    parameters, \n",
    "    cv=all_splits, # pass an iterable\n",
    "    #scoring='neg_mean_absolute_percentage_error',\n",
    "    n_jobs=8, \n",
    "    verbose=1\n",
    ").fit(X_train, y_train.values.ravel())\n",
    " \n",
    "print('Training set score: ' + str(grid.score(X_train, y_train.values.ravel())))\n",
    "print('Validation set score: ' + str(grid.score(X_valid, y_valid.values.ravel())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the best set of parameters\n",
    "best_params = grid.best_params_\n",
    "print(best_params)\n",
    "# Stores the optimum model in best_pipe\n",
    "best_pipe = grid.best_estimator_\n",
    "print(best_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(grid.cv_results_, orient='columns')\n",
    "print(result_df.columns)\n",
    "result_df.sort_values(by='rank_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor = best_pipe.steps[1][1]\n",
    "\n",
    "importances = best_regressor.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_regressor.estimators_], axis=0) # check all trees inside best regressor ensamble\n",
    "\n",
    "forest_importances = pd.Series(importances, index= X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.barh(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=result_df,\n",
    "            kind='line',\n",
    "            x='param_randomforestregressor__n_estimators',\n",
    "            y='mean_test_score',\n",
    "            hue='param_randomforestregressor__min_samples_leaf',\n",
    "            col='param_randomforestregressor__max_depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, X, y, cv):\n",
    "#     cv_results = cross_validate(\n",
    "#         model,\n",
    "#         X,\n",
    "#         y,\n",
    "#         cv=cv,\n",
    "#         scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n",
    "#     )\n",
    "#     mae = -cv_results[\"test_neg_mean_absolute_error\"]\n",
    "#     rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n",
    "#     print(\n",
    "#         f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
    "#         f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# evaluate(experimental_pipeline, X, y.values.ravel(), cv=ts_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(y_valid)\n",
    "y_valid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(y_train[:200])\n",
    "y_train.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_predictions = best_pipe.predict(X_train)\n",
    "valid_predictions = best_pipe.predict(X_valid)\n",
    "\n",
    "viz_tools.plot_timeseries(\n",
    "    ts_list=[\n",
    "        y_train, \n",
    "        pd.Series(train_set_predictions, index=y_train.index), \n",
    "        y_valid, \n",
    "        pd.DataFrame(valid_predictions, index=y_valid.index)\n",
    "    ], \n",
    "    #primary_axis=[True],#,True,True,True], \n",
    "    legend=['y_training', 'sample_pred', 'y_validation', 'validation_pred'],\n",
    "    sample_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_predictions = best_pipe.predict(X_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_train, train_set_predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_valid, valid_predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_valid.min(), y_valid.max()], [y_valid.min(), y_valid.max()], \"k--\", lw=4)\n",
    "ax.set_xlabel(\"Measured\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=1000, oob_score=True, random_state=123, max_depth=10, n_jobs=8)\n",
    "regressor.fit(X_train.values, y_train.values)\n",
    "y_pred = regressor.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_error = mean_squared_error(y_train['y'], X_train)\n",
    "\n",
    "# test_error = mean_squared_error(y_test['y'], X_test)\n",
    "\n",
    "prediction_error = mean_squared_error(y_test['y'], y_pred)\n",
    "\n",
    "print(prediction_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = regressor.feature_importances_\n",
    "pd.Series(importances, index=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "fig = px.line()\n",
    "fig.add_scatter(y=pd.Series(y_pred), name='pred')\n",
    "fig.add_scatter(y=y_test['y'], name='real')\n",
    "# px.line(pd.DataFrame(y_pred.T))\n",
    "# px.line(pd.DataFrame(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line()\n",
    "y_sample_pred = regressor.predict(X_train.values)\n",
    "fig.add_scatter(y=pd.Series(y_train['y'].values), name='real')\n",
    "fig.add_scatter(y=y_sample_pred, name='sample_prediction')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing different BB strategies\n",
    "# frequencies = ('5min', '10min', '30min', '60min', '120min')\n",
    "# window_devs = np.arange(0.4,4.1, 0.2)\n",
    "# bb_combinations = list(itertools.product(np.arange(10,200, 5), window_devs, frequencies))\n",
    "# strategy_performance = []\n",
    "# for bb_combination in bb_combinations:\n",
    "\n",
    "#     window = bb_combination[0]\n",
    "#     window_dev = np.round(bb_combination[1], 3)\n",
    "#     freq = bb_combination[2]\n",
    "#     print(bb_combination)\n",
    "\n",
    "#     trading_strategy = TradingStrategy(df_px, frequency=freq)\n",
    "#     trading_strategy.resample_data() # resampling\n",
    "\n",
    "\n",
    "#     trading_strategy.add_strategy(\n",
    "#         'BollingerBandsLO', \n",
    "#         execution_type='current_bar_close',#'next_bar_open', 'current_bar_close, 'cheat_previous_close\n",
    "#         stop_loss=0.0,\n",
    "#         comms_bps=50,\n",
    "#         window=window,\n",
    "#         window_dev=window_dev,\n",
    "#         print_trades=False\n",
    "#     )\n",
    "\n",
    "\n",
    "#     try: strategy_return = trading_strategy.trades_df['cum_trades_pctg_return'][-1]\n",
    "#     except: strategy_return = np.nan\n",
    "#     strategy_performance.append([bb_combination[0], bb_combination[1], bb_combination[2], strategy_return])\n",
    "\n",
    "# strategy_performance_df_bb = pd.DataFrame(strategy_performance, columns=['bb_window', 'bb_window_devs', 'frequency', 'performance']).sort_values('performance', ascending=False)\n",
    "# strategy_performance_df_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df['ma_diff'] = (trading_strategy.df['high'] - trading_strategy.df['high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add BB and maybe another strategy for comparison\n",
    "# implement capturing of data snapped - a simple database?\n",
    "# make sure engine works as expected\n",
    "# keep monitoring bot\n",
    "# make sure work on grouper and performances and transaction cost holds\n",
    "# why returns prices are different than close? slippage?\n",
    "\n",
    "\n",
    "# NOTE on data gathered:\n",
    "# orders are executed one bar after triggered. Whilst that's ok for for buy, is that fine for sells?\n",
    "# issue with API not finding order executed sometimes: do we need to fetch all the time?\n",
    "# no need to have stop loss price printed after closing position\n",
    "\n",
    "# window = 45\n",
    "# window_dev = 1.4\n",
    "\n",
    "\n",
    "## Exchange connectivity\n",
    "exchange = ccxt.bitstamp(\n",
    "    {\n",
    "        'apiKey': config.BITSTAMP_API_KEY,\n",
    "        'secret': config.BITSTAMP_API_SECRET\n",
    "    }\n",
    ")\n",
    "\n",
    "# bars = exchange.fetch_ohlcv('BTC/GBP', timeframe='30m', limit=1000)\n",
    "# bars_df = pd.DataFrame(bars, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "# bars_df['timestamp'] = pd.to_datetime(bars_df['timestamp'], unit='ms')\n",
    "# bars_df.index = pd.DatetimeIndex(bars_df['timestamp'], freq='30min')\n",
    "\n",
    "short_ema = 10\n",
    "long_ema = 220\n",
    "trading_strategy = TradingStrategy(df_px, frequency='30min') # df_px\n",
    "trading_strategy.resample_data() # resampling\n",
    "trading_strategy.df['bar_volatility'] = ((trading_strategy.df['high'] - trading_strategy.df['low']) / trading_strategy.df['close']).rolling(20).mean()\n",
    "# trading_strategy.add_indicator('BollingerBands', window=20)\n",
    "\n",
    "\n",
    "\n",
    "trading_strategy.add_strategy(\n",
    "    'EMACrossOverLO', # EMACrossOverLO BollingerBandsLO\n",
    "    execution_type='current_bar_close',#'next_bar_open', 'current_bar_close, 'cheat_previous_close\n",
    "    stop_loss_bps=1000,\n",
    "    comms_bps=25,\n",
    "    short_ema=short_ema,\n",
    "    long_ema=long_ema,\n",
    "    print_trades=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trading_strategy.trading_chart(\n",
    "    plot_strategy=True,\n",
    "    plot_volatility=True,\n",
    "    short_ema=f'ema_{short_ema}', \n",
    "    long_ema=f'ema_{long_ema}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trading_strategy.trading_chart(\n",
    "    plot_strategy=True,\n",
    "    plot_volatility=True,\n",
    "    short_ema=f'ema_{short_ema}', \n",
    "    long_ema=f'ema_{long_ema}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = trading_strategy.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df['EMACrossOverLO_signal'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df.shift(1).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df['open_close_var'] = ((trading_strategy.df['close'] - trading_strategy.df['open']) / trading_strategy.df['close']).rolling(48*7).mean()\n",
    "trading_strategy.df['high_low_var'] = ((trading_strategy.df['high'] - trading_strategy.df['low']) / trading_strategy.df['close']).rolling(48*7).mean()\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, \n",
    "    cols=1,\n",
    "    shared_xaxes=True,\n",
    "    row_heights=[0.5, 0.5],\n",
    "    # vertical_spacing=0.02,\n",
    "    specs=[\n",
    "        [{\"secondary_y\": False}],\n",
    "        [{\"secondary_y\": True}]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_scatter(x=trading_strategy.df.index, y=trading_strategy.df['high_low_var'], row=2, col=1, name='high_low')\n",
    "fig.add_scatter(x=trading_strategy.df.index, y=trading_strategy.df['open_close_var'], secondary_y=True,row=2, col=1, name='open_close')\n",
    "fig.add_scatter(x=trading_strategy.df.index, y=trading_strategy.df['close'], row=1, col=1, name='price')\n",
    "\n",
    "fig.update_layout(height=1000)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trading_strategy.df['open'] - trading_strategy.df['close']) / trading_strategy.df['close']).rolling(48*7).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((trading_strategy.df['high'] - trading_strategy.df['low']) / trading_strategy.df['close']).rolling(48*7).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df['ma_diff'] = (trading_strategy.df['ema_20'] - trading_strategy.df['ema_110']) / (((trading_strategy.df['ema_20'] + trading_strategy.df['ema_110']))/2)\n",
    "trading_strategy.df['ma_diff'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy = TradingStrategy(df_px, frequency='30min')\n",
    "trading_strategy.resample_data()\n",
    "trading_strategy.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy = TradingStrategy(df_px, frequency='120min')\n",
    "trading_strategy.resample_data() # resampling\n",
    "\n",
    "short_ema = 20\n",
    "long_ema = 30\n",
    "\n",
    "trading_strategy.add_strategy(\n",
    "    'EMACrossOverLO', \n",
    "    execution_type='current_bar_close',#'next_bar_open', 'current_bar_close, 'cheat_previous_close\n",
    "    stop_loss=0.0,\n",
    "    comms_bps=50,\n",
    "    short_ema=short_ema,\n",
    "    long_ema=long_ema,\n",
    "    print_trades=False\n",
    ")\n",
    "trading_strategy.trading_chart(plot_strategy=True, short_ema=f'ema_{short_ema}', long_ema=f'ema_{long_ema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.trades_df#['exit_price'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.trades_df[:].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df.head(60)[['close','EMACrossOverLO_new_position', 'trade_grouper', 'px_returns_calcs', 'gross_log_returns', 'EMACrossOverLO_gross_log_returns']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_trades = exchange.fetchMyTrades(pair)\n",
    "open_orders = exchange.fetchOpenOrders(pair)\n",
    "past_trades, open_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df = trading_strategy.df.groupby('trade_grouper').agg(\n",
    "    entry_price=('execution_price', 'first'), \n",
    "    exit_price=('execution_price', 'last'), \n",
    "    trade_len=('trade_grouper', 'count'),\n",
    "    direction=('EMACrossOverLO_new_position', 'first'),\n",
    "    liquidated_at=('execution_time', 'last')\n",
    ")\n",
    "\n",
    "trades_df['trades_log_return'] = np.log(trades_df['exit_price']) - np.log(trades_df['entry_price'])\n",
    "trades_df['cum_trades_log_return'] = trades_df['trades_log_return'].cumsum()\n",
    "\n",
    "trades_df['trades_pctg_return'] = np.exp(trades_df['trades_log_return']) - 1\n",
    "trades_df['cum_trades_pctg_return'] = np.exp(trades_df['cum_trades_log_return']) - 1\n",
    "\n",
    "cum_return = f\"{trades_df['cum_trades_pctg_return'][-1]:.2%}\"\n",
    "\n",
    "trades_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add trade profitability TODO: profitability dot not perfecty alligned\n",
    "# 0 on stop loss period impacting returns\n",
    "# probably need a refactoring to accomodate for multiple trades in the same period\n",
    "# proceede with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading_strategy.df[(trading_strategy.df['EMACrossOver_new_position']!=0)|(trading_strategy.df['sl_hit']!=0)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_strategy.df.to_excel(f'StratTest/Exports/{trading_strategy.strategy}_{trading_strategy.stop_loss}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare df trades\n",
    "\n",
    "# get positions in the dataframe where indicator generates signals\n",
    "open_trades_idx = np.where(df['ema_cross_position']!=0)[0]\n",
    "# -2 because of shape is n rows and df is 0 indexed and because we do + 1 later - avoid out of bound error\n",
    "closing_trades_idx = np.append(open_trades_idx, df.shape[0]-2)[1:] \n",
    "df_trades = df.iloc[open_trades_idx][['ema_cross_position']].copy() # empty dataframe with only datetime index\n",
    "\n",
    "# entry and closing points\n",
    "df_trades['entry_price'] = df.iloc[open_trades_idx+1]['open'].values # assume entry trade is executed at the next bar open\n",
    "df_trades['closing_price'] = df.iloc[closing_trades_idx+1]['open'].values # assume closing is executed at the next bar open\n",
    "\n",
    "# trade discrete returns\n",
    "df_trades['discrete_return'] = df_trades['ema_cross_position'] * ((df_trades['closing_price'] / df_trades['entry_price']) - 1)\n",
    "\n",
    "# how long are the trades \n",
    "df_trades['trade_n_periods'] = closing_trades_idx - open_trades_idx\n",
    "df_trades['trade_duration'] = df.iloc[closing_trades_idx].index - df.iloc[open_trades_idx].index\n",
    "\n",
    "# what happened throughout the trade\n",
    "df['trade_grouper'] = np.nan\n",
    "df.loc[df.iloc[open_trades_idx].index, 'trade_grouper'] = df.iloc[open_trades_idx].index\n",
    "df['trade_grouper'] = df['trade_grouper'].fillna(method='ffill')\n",
    "df.head(60)\n",
    "\n",
    "all_trades_list = []\n",
    "for name, sub_df in df.groupby(by='trade_grouper'):\n",
    "    max_val = sub_df['high'].max()\n",
    "    min_val = sub_df['low'].min()\n",
    "    returns_std = sub_df['returns'].std()\n",
    "\n",
    "    all_trades_list.append([name, max_val, min_val, returns_std])\n",
    "\n",
    "\n",
    "intra_trade_stats = pd.DataFrame(all_trades_list, columns=['datetime', 'px_high', 'px_low', 'returns_std']).set_index('datetime')\n",
    "df_trades = pd.merge(df_trades, intra_trade_stats, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "def max_dd_pctg(row):\n",
    "    ''' Measure of how \"painful\" holding the trade was '''\n",
    "    if row['ema_cross_position'] == 1:\n",
    "        return (row['entry_price'] - row['px_low'])/row['px_low']\n",
    "    elif row['ema_cross_position'] == -1:\n",
    "        return (-(row['entry_price'] - row['px_high']))/row['px_high']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_trades['dd_pctg'] = df_trades.apply(max_dd_pctg, axis=1)\n",
    "\n",
    "# calculate trade returns and jump into risk management / stop losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sl_trigger_time = sub_df[~(sub_df['sl_trigger'] < sub_df['low'])].index\n",
    "\n",
    "# shortened trade time due to stop loss\n",
    "stopped_sub_df = sub_df[sub_df.index<=sl_trigger_time[0]].copy()\n",
    "stopped_sub_df['strategy_position'][-1] = -1\n",
    "\n",
    "# remaining part of the trade, now position need to change to 0\n",
    "quitted_sub_df = sub_df[sub_df.index>=sl_trigger_time[0]].copy()\n",
    "quitted_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.loc[sl_trigger_time, 'strategy_position'] = -1\n",
    "sub_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[sub_df.index<=sl_trigger_time[0]]['strategy_position'][-1] = -1\n",
    "sub_df[sub_df.index<=sl_trigger_time[0]]['strategy_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_price(row):\n",
    "    ''' Get worst price relative to position '''\n",
    "    if row['ema_cross_signal'] > 0:\n",
    "        return min(row['open'], row['high'], row['low'], row['close'])\n",
    "    elif row['ema_cross_signal'] < 0:\n",
    "        return max(row['open'], row['high'], row['low'], row['close'])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "# static stop\n",
    "\n",
    "sub_df['worst_price_timestamp'] = sub_df.apply(get_worst_price, axis=1)\n",
    "# calculate loss vs worst price over the period\n",
    "sub_df['cumulative_performance'] = sub_df['ema_cross_returns'].cumsum()\n",
    "sub_df['worst_period_potential_loss'] = sub_df['ema_cross_signal'] * ((sub_df['worst_price_timestamp'] / entry_price) - 1)\n",
    "\n",
    "sub_df[['ema_cross_returns', 'cumulative_performance', 'worst_period_potential_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades.apply(lambda x: (x['closing_price'] / x['entry_price']) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics\n",
    "# Net Profit\n",
    "net_profit = df['ema_cross_cash'][-1] - initial_cash \n",
    "\n",
    "# Max Drowdown\n",
    "max_dd = df_trades['dd_pctg'].max()\n",
    "\n",
    "# Win Ratio\n",
    "win_ratio = (df_trades['discrete_return']>0).sum() / df_trades.shape[0]\n",
    "\n",
    "print(f'Net Profit: {net_profit:.2f}, Max Drawdown: {max_dd:.2%}, Win Ratio: {win_ratio:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df['close'].plot(legend=True)\n",
    "# ((np.exp(df['ema_cross_returns'].cumsum()) * 100)).plot(legend=True)\n",
    "# # ((np.exp(df['returns'].cumsum()) * df['close'][0])).plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ema_cross_position'].cumsum().plot()\n",
    "# df['ema_cross_signal'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.index>='2020-11-22 21:00:00'].head(50)[['trade_grouper', 'trade_grouper', 'close', 'low', 'high', 'sl_trigger', 'ema_cross_new_position', 'ema_cross_signal', 'ema_cross_trades', 'strategy_new_position', 'strategy_signal', 'strategy_trades']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import schedule\n",
    "from datetime import datetime\n",
    "import config\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import StratTest.bot as bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Exchange connectivity\n",
    "# exchange = ccxt.binance(\n",
    "#     {\n",
    "#         'apiKey': config.BINANCE_API_KEY,\n",
    "#         'secret': config.BINANCE_SECRET_KEY\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# markets = exchange.load_markets()\n",
    "# pair = 'BTC/USD'\n",
    "\n",
    "# bars = exchange.fetch_ohlcv(pair, timeframe='1m', limit=100) # most recent candle keeps evolving\n",
    "\n",
    "# exchange.fetch_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 'BTC/USDT'\n",
    "strategy = 'EMACrossOver'\n",
    "indicator = 'EMAIndicator'\n",
    "short_ema = 10\n",
    "long_ema = 20\n",
    "my_bot = bot.TradingBot('EMACrossOverLO', indicator, sandbox=False, short_ema=short_ema, long_ema=long_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = my_bot.exchange.fetch_ohlcv('BTC/GBP', timeframe='30m', limit=100) # most recent candle keeps evolving\n",
    "my_bot.bars_df = pd.DataFrame(bars, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "my_bot.bars_df['timestamp'] = pd.to_datetime(my_bot.bars_df['timestamp'], unit='ms')\n",
    "indicator_df = my_bot._get_crossover()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bot.bars_df['ema_10'].plot()\n",
    "my_bot.bars_df['ema_20'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bot.exchange.has['cancelOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_px.iloc[-1].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bot.bars_df.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'BTC/GBP'\n",
    "\n",
    "order_book = my_bot.exchange.fetchOrderBook(symbol)\n",
    "\n",
    "ob_datetime = order_book['datetime']\n",
    "\n",
    "top_ask_px = order_book['asks'][0][0]\n",
    "top_ask_quantity = order_book['asks'][0][1]\n",
    "\n",
    "top_bid_px = order_book['bids'][0][0]\n",
    "top_bid_quantity = order_book['bids'][0][1]\n",
    "\n",
    "top_mid_px = (top_ask_px + top_bid_px) / 2\n",
    "top_ob_spread = (top_ask_px - top_bid_px) / top_mid_px\n",
    "\n",
    "# current_mid_price : 1 BTC = my_size : x BTC\n",
    "my_size_gbp = 22\n",
    "trade_size = my_size_gbp/(top_mid_px)\n",
    "trade_size, top_bid_px, top_ask_px, ob_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 'BTC/GBP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_orders = my_bot.exchange.fetchMyTrades(pair)\n",
    "[order['order'] for order in executed_orders if order['order']=='1432301048147968']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_trades = exchange.fetchMyTrades(pair)\n",
    "open_orders = exchange.fetchOpenOrders(pair)\n",
    "past_trades, open_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances = my_bot.exchange.fetchBalance()\n",
    "balances['GBP'], balances['BTC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bot.exchange.has['createLimitOrder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_orders = my_bot.exchange.fetchOpenOrders('BTC/GBP') # open orders\n",
    "settled_trades = my_bot.exchange.fetchMyTrades('BTC/GBP') # provides the history of settled trades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bot.exchange.fetchBalance()['GBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import AlternativeData.reddit_sql as reddit_sql\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = './AlternativeData/reddit_test.db'\n",
    "reddit_sql.create_psaw_table(db_path)\n",
    "reddit_sql.create_praw_tables(db_path)\n",
    "reddit_sql.get_all_db_tables(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "c.execute('''DROP TABLE reddit_praw_submissions''')\n",
    "c.execute('''DROP TABLE reddit_praw_comments''')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime=datetime(2022, 1, 1, 0, 0, 0)\n",
    "end_datetime=datetime(2022, 1, 16, 12, 0, 0)\n",
    "freq='12H'\n",
    "daterange = pd.date_range(start_datetime, end_datetime, freq=freq)\n",
    "\n",
    "subreddits = ['wallstreetbets','cryptcurrency', 'bitcoin']\n",
    "\n",
    "fields_list = ['id','author', 'created_utc', 'subreddit', 'title', 'selftext', 'full_link']\n",
    "\n",
    "# TODO add retries\n",
    "for subreddit in subreddits:\n",
    "\n",
    "    for tstmp in daterange:\n",
    "\n",
    "        print(f'Fetching {subreddit} data: start: {tstmp}, end: {tstmp + pd.Timedelta(freq)}')\n",
    "\n",
    "        start_epoch = int(tstmp.timestamp())\n",
    "        end_epoch = int((tstmp + pd.Timedelta(freq)).timestamp())\n",
    "\n",
    "        submissions = api.search_submissions(\n",
    "            after=start_epoch,\n",
    "            before=end_epoch,\n",
    "            subreddit=subreddit,\n",
    "            filter=fields_list,\n",
    "            limit=None\n",
    "        )\n",
    "\n",
    "        print('storing..')\n",
    "        for submission in submissions:\n",
    "                try:\n",
    "                    reddit_sql.insert_new_psaw_submission(submission, db=db_path)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "        print('~~~ moving on ~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_output = reddit_sql.get_reddits('reddit_psaw_submissions', db=db_path)\n",
    "test_query_df = pd.DataFrame(test_query_output, columns=fields_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config.reddit_personal_use_script,\n",
    "    client_secret=config.reddit_secret,\n",
    "    # password=config.reddit_password,\n",
    "    user_agent=\"rlt_bot\",\n",
    "    # username=config.reddit_username,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.link_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=config.reddit_personal_use_script,\n",
    "    client_secret=config.reddit_secret,\n",
    "    # password=config.reddit_password,\n",
    "    user_agent=\"rlt_bot\",\n",
    "    # username=config.reddit_username,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = reddit.submission('eq4prj')\n",
    "# for comment in submission.comments.list():\n",
    "#     try:\n",
    "#         print(comment.link_id)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id=sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id=sub_id)\n",
    "\n",
    "reddit_sql.insert_new_praw_submission(submission, db=db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: submission.body\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[415].deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psaw_ids = pd.DataFrame(reddit_sql.get_reddits('reddit_psaw_submissions', db=db_path))[0]\n",
    "praw_ids = pd.DataFrame(reddit_sql.get_reddits('reddit_praw_submissions', db=db_path))[0]\n",
    "subs_list = psaw_ids[~psaw_ids.isin(praw_ids)].tolist()\n",
    "\n",
    "\n",
    "# praw database update control flow\n",
    "counter = 0\n",
    "\n",
    "for sub_id in subs_list:\n",
    "\n",
    "    try:\n",
    "        if sub_id not in pd.DataFrame(reddit_sql.get_reddits('reddit_praw_submissions', db=db_path))[0].tolist():\n",
    "            print(f'Working on {sub_id}')\n",
    "            submission = reddit.submission(id=sub_id)\n",
    "\n",
    "            reddit_sql.insert_new_praw_submission(submission, db=db_path)\n",
    "\n",
    "            all_comments = submission.comments.list()\n",
    "            print(f'number of comments: {len(all_comments)}')\n",
    "\n",
    "            if len(all_comments)>0:\n",
    "                # TODO some comments return no attributes, investigate whether\n",
    "                # these are deleted comments or something else is going on\n",
    "                for comment in all_comments:\n",
    "\n",
    "                    reddit_sql.insert_new_praw_comment(comment, db=db_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'{e} ### submission id: {sub_id}#') ### {submission.url}')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_columns = ['praw_comment_id', 'praw_comment_parent_id', 'praw_comment_link_id', 'praw_comment_created_utc','praw_comment_body','praw_comment_score']\n",
    "all_comments = pd.DataFrame(reddit_sql.get_reddits('reddit_praw_comments', db=db_path), columns=comment_columns)\n",
    "print(all_comments.shape)\n",
    "all_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments.groupby('praw_comment_link_id')['praw_comment_id'].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments[all_comments['praw_comment_link_id'] == 't3_eq4prj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[9].created_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[9].link_id, submission.comments.list()[9].parent_id, submission.comments.list()[9].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[7].link_id, submission.comments.list()[7].parent_id, submission.comments.list()[7].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[4].link_id, submission.comments.list()[4].parent_id, submission.comments.list()[4].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[1].link_id, submission.comments.list()[1].parent_id, submission.comments.list()[1].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[9].parent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.comments.list()[7].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission.comments.replace_more(limit=None)\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)\n",
    "    print()\n",
    "    print('##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(submissions))\n",
    "data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'reddit_testing.db'\n",
    "reddit_sql.create_subreddit_table(db=f'./AlternativeData/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d_'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth(config.reddit_personal_use_script, config.reddit_secret)\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': config.reddit_username,\n",
    "        'password': config.reddit_password}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'rlt_bot/0.0.1'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = requests.get(\"https://oauth.reddit.com/r/python/hot\",\n",
    "#                    headers=headers)\n",
    "\n",
    "# print(res.json())  # let's see what we get\n",
    "\n",
    "# for post in res.json()['data']['children']:\n",
    "#     print(post['data']['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subreddit_df(res):\n",
    "\n",
    "    df = pd.DataFrame()  # initialize dataframe\n",
    "\n",
    "    # loop through each post retrieved from GET request\n",
    "    for post in res.json()['data']['children']:\n",
    "        # append relevant data to dataframe\n",
    "        df = df.append({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'author': post['data']['author'],\n",
    "            'link_flair_css_class': post['data']['link_flair_css_class'],\n",
    "            'created_utc': post['data']['created_utc'],\n",
    "            'created': post['data']['created'],\n",
    "            'edited': post['data']['edited'],\n",
    "            'kind': post['kind'], # ie t3 - link\n",
    "            'id': post['data']['id']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], utc=True, unit='s')\n",
    "    df['created'] = pd.to_datetime(df['created'], utc=None, unit='s')\n",
    "    # df['edited'] = pd.to_datetime(df['edited'], utc=None, unit='s') # 0 to be replaced with NAt\n",
    "\n",
    "    # sort to make sure that oldest entry is at the bottom\n",
    "    df = df.sort_values(by='created_utc', ascending=False) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'CryptoCurrency' # 'ETFs, CryptoCurrency\n",
    "listing = 'new'\n",
    "params = {'limit': 100}\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# loop through 10 times (returning 1K posts)\n",
    "for i in range(13):\n",
    "    print(f'{i} iteration')\n",
    "    try:\n",
    "        # make a request for the trending posts in /r/[subreddit]\n",
    "        res = requests.get(\n",
    "            f\"https://oauth.reddit.com/r/{subreddit}/{listing}\",\n",
    "            headers=headers,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = subreddit_df(res)\n",
    "        new_df['bot_iteration'] = i\n",
    "\n",
    "        # take the final row (oldest entry)\n",
    "        row = new_df.iloc[new_df.shape[0]-1]\n",
    "        # create fullname\n",
    "        # fullname = row['kind'] + '_' + row['id']\n",
    "        # print(fullname)\n",
    "        \n",
    "        # add/update fullname in params\n",
    "        params['after'] = res.json()['data']['after']#fullname\n",
    "        \n",
    "        # append new_df to data\n",
    "        final_df = final_df.append(new_df, ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(res.json())\n",
    "        # time.sleep(60)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df['id']=='s3orms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sort_values(by='created_utc')['id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = requests.get(\n",
    "#     f\"https://oauth.reddit.com/r/{subreddit}/{listing}\",\n",
    "#     headers=headers,\n",
    "#     params=params\n",
    "# )\n",
    "# res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_new_reddit(post):\n",
    "\n",
    "    # using conn as a context manager to avoid explicitly committing every time\n",
    "    with conn:\n",
    "        values = {\n",
    "            'author':post['author'],\n",
    "            'id':post['id'],\n",
    "            'created_utc':str(post['created_utc']),\n",
    "            'subreddit':post['subreddit'],\n",
    "            'title':post['title'],\n",
    "            'selftext':post['selftext'],\n",
    "            'ups':post['ups'],\n",
    "            'downs':post['downs'],\n",
    "            'upvote_ratio':post['upvote_ratio']\n",
    "        }\n",
    "\n",
    "        c.execute(f'''\n",
    "            INSERT INTO reddit_test VALUES (\n",
    "                :author,\n",
    "                :id,\n",
    "                :created_utc,\n",
    "                :subreddit,\n",
    "                :title,\n",
    "                :selftext,\n",
    "                :ups,\n",
    "                :downs,\n",
    "                :upvote_ratio\n",
    "            )\n",
    "        ''', values)\n",
    "\n",
    "\n",
    "def get_reddits(ups_threshold):\n",
    "    c.execute(\"SELECT * FROM reddit_test WHERE ups>=:ups\", {'ups':ups_threshold})\n",
    "    return c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(':memory:') # can pass a file 'filename.db' or make an in memory db - ':memory:' # './local_data/text_data.db'\n",
    "c = conn.cursor() # create a cursor\n",
    "\n",
    "\n",
    "c.execute('''\n",
    "    CREATE TABLE reddit_test (\n",
    "        author text,\n",
    "        id text,\n",
    "        created_utc text,\n",
    "        subreddit text,\n",
    "        title text,\n",
    "        selftext text,\n",
    "        ups integer,\n",
    "        downs integer,\n",
    "        upvote_ratio real\n",
    "    )\n",
    "''')\n",
    "conn.commit()\n",
    "\n",
    "# TODO use execute many instead for bulk upload\n",
    "[insert_new_reddit(entry) for entry in final_df.to_dict('records')]\n",
    "\n",
    "reddits = get_reddits(0)\n",
    "columns = [d[0] for d in c.description]\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(reddits, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c128b2642c5e45a6a6e517f84a10ebf4883d575b0402fafd309361fdab983f20"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('rltrader_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
