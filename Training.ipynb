{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python (rltrader_env)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from func_tools import import_px_data, standardize, fetch_s3_trade_files, cnn_data_reshaping, reshape_lob_levels, back_to_labels, intraday_vol_ret \n",
    "import visualization_tools as viz_t\n",
    "from labelling_class import Labels_Generator, cleaned_labels, label_insights, get_strategy_pnl\n",
    "\n",
    "import inspect\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, BatchNormalization, LeakyReLU, concatenate, add, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LOBData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict tf to only allocate xGB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other inputs\n",
    "length = 100\n",
    "batch_size = 64\n",
    "\n",
    "start_plot = 0\n",
    "end_plot = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 5 µs, sys: 4 µs, total: 9 µs\nWall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiments_folder = 'Experiments'\n",
    "frequency = timedelta(seconds=10)\n",
    "pair = 'BTC_AAVE'\n",
    "date_start = '2020_11_01'\n",
    "date_end = '2021_04_07'\n",
    "lob_depth = 10\n",
    "norm_type = 'dyn_z_score'\n",
    "roll = 7200 * 6\n",
    "label_technique = 'three_steps'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "onal data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-16.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-17.csv.gz\n",
      "75198 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-17.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-18.csv.gz\n",
      "75107 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-18.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-19.csv.gz\n",
      "74242 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-19.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-20.csv.gz\n",
      "73537 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-20.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-21.csv.gz\n",
      "74828 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-21.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-22.csv.gz\n",
      "75025 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-22.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-23.csv.gz\n",
      "75160 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-23.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-24.csv.gz\n",
      "73722 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-24.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-25.csv.gz\n",
      "73709 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-25.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-26.csv.gz\n",
      "73891 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-26.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-27.csv.gz\n",
      "74749 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-27.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-28.csv.gz\n",
      "74278 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-28.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-29.csv.gz\n",
      "74059 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-29.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-30.csv.gz\n",
      "74264 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-30.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2020-12-31.csv.gz\n",
      "75014 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2020-12-31.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-01.csv.gz\n",
      "74572 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-01.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-02.csv.gz\n",
      "74202 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-02.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-03.csv.gz\n",
      "74542 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-03.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-04.csv.gz\n",
      "74828 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-04.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-05.csv.gz\n",
      "74502 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-05.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-06.csv.gz\n",
      "75529 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-06.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-07.csv.gz\n",
      "74785 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-07.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-08.csv.gz\n",
      "74550 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-08.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-09.csv.gz\n",
      "74103 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-09.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-10.csv.gz\n",
      "74053 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-10.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-11.csv.gz\n",
      "75809 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-11.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-12.csv.gz\n",
      "74789 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-12.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-13.csv.gz\n",
      "75081 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-13.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-14.csv.gz\n",
      "74806 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-14.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-15.csv.gz\n",
      "74013 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-15.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-16.csv.gz\n",
      "73501 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-16.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-17.csv.gz\n",
      "74286 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-17.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-18.csv.gz\n",
      "73589 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-18.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-19.csv.gz\n",
      "76714 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-19.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-20.csv.gz\n",
      "73449 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-20.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-21.csv.gz\n",
      "73403 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-21.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-22.csv.gz\n",
      "71240 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-22.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-23.csv.gz\n",
      "72277 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-23.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-24.csv.gz\n",
      "72714 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-24.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-25.csv.gz\n",
      "73253 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-25.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-26.csv.gz\n",
      "73502 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-26.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-27.csv.gz\n",
      "74245 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-27.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-28.csv.gz\n",
      "74180 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-28.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-29.csv.gz\n",
      "74124 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-29.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-30.csv.gz\n",
      "74610 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-30.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-01-31.csv.gz\n",
      "74242 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-01-31.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-01.csv.gz\n",
      "73555 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-01.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-02.csv.gz\n",
      "74560 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-02.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-03.csv.gz\n",
      "73275 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-03.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-04.csv.gz\n",
      "73987 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-04.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-05.csv.gz\n",
      "73425 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-05.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-06.csv.gz\n",
      "73278 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-06.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-07.csv.gz\n",
      "73953 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-07.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-08.csv.gz\n",
      "74150 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-08.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-09.csv.gz\n",
      "73327 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-09.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-10.csv.gz\n",
      "72964 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-10.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-11.csv.gz\n",
      "73425 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-11.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-12.csv.gz\n",
      "72808 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-12.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-13.csv.gz\n",
      "73448 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-13.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-14.csv.gz\n",
      "73360 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-14.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-15.csv.gz\n",
      "73203 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-15.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-16.csv.gz\n",
      "73332 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-16.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-17.csv.gz\n",
      "70341 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-17.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-18.csv.gz\n",
      "72915 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-18.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-19.csv.gz\n",
      "71934 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-19.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-20.csv.gz\n",
      "74067 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-20.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-21.csv.gz\n",
      "73797 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-21.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-22.csv.gz\n",
      "72644 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-22.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-23.csv.gz\n",
      "73116 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-23.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-24.csv.gz\n",
      "73182 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-24.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-25.csv.gz\n",
      "74671 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-25.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-26.csv.gz\n",
      "73498 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-26.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-27.csv.gz\n",
      "74009 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-27.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-02-28.csv.gz\n",
      "72976 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-02-28.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-01.csv.gz\n",
      "74298 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-01.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-02.csv.gz\n",
      "74136 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-02.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-03.csv.gz\n",
      "73648 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-03.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-04.csv.gz\n",
      "73261 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-04.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-05.csv.gz\n",
      "73568 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-05.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-06.csv.gz\n",
      "71751 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-06.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-07.csv.gz\n",
      "73181 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-07.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-08.csv.gz\n",
      "73126 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-08.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-09.csv.gz\n",
      "73901 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-09.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-10.csv.gz\n",
      "74102 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-10.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-11.csv.gz\n",
      "73994 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-11.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-12.csv.gz\n",
      "73552 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-12.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-13.csv.gz\n",
      "74379 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-13.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-14.csv.gz\n",
      "73882 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-14.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-15.csv.gz\n",
      "73875 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-15.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-16.csv.gz\n",
      "73967 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-16.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-17.csv.gz\n",
      "73867 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-17.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-18.csv.gz\n",
      "72977 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-18.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-19.csv.gz\n",
      "72638 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-19.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-20.csv.gz\n",
      "73046 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-20.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-21.csv.gz\n",
      "72750 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-21.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-22.csv.gz\n",
      "73362 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-22.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-23.csv.gz\n",
      "73081 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-23.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-24.csv.gz\n",
      "72652 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-24.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-25.csv.gz\n",
      "73089 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-25.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-26.csv.gz\n",
      "73713 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-26.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-27.csv.gz\n",
      "72642 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-27.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-28.csv.gz\n",
      "72435 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-28.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-29.csv.gz\n",
      "73225 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-29.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-30.csv.gz\n",
      "73359 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-30.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-03-31.csv.gz\n",
      "73112 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-03-31.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-01.csv.gz\n",
      "73309 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-01.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-02.csv.gz\n",
      "72867 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-02.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-03.csv.gz\n",
      "73028 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-03.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-04.csv.gz\n",
      "73198 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-04.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-05.csv.gz\n",
      "73621 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-05.csv.gz\n",
      "Generating Experiments/cache/BTC_AAVE/10_levels/10s/2021-04-06.csv.gz\n",
      "73061 additional data points in Experiments/cache/BTC_AAVE/10_levels/original_frequency/2021-04-06.csv.gz\n",
      "CPU times: user 33min 57s, sys: 30.2 s, total: 34min 27s\n",
      "Wall time: 34min 27s\n",
      "/usr/lib/python3/dist-packages/dask/dataframe/io/csv.py:381: UserWarning: Warning gzip compression does not support breaking apart files\n",
      "Please ensure that each individual file can fit in memory and\n",
      "use the keyword ``blocksize=None to remove this message``\n",
      "Setting ``blocksize=None``\n",
      "  warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                Unnamed: 0 Datetime  Level Ask_Price Ask_Size Bid_Price Bid_Size Sequence\n",
       "npartitions=148                                                                          \n",
       "                     int64   object  int64   float64  float64   float64  float64    int64\n",
       "                       ...      ...    ...       ...      ...       ...      ...      ...\n",
       "...                    ...      ...    ...       ...      ...       ...      ...      ...\n",
       "                       ...      ...    ...       ...      ...       ...      ...      ...\n",
       "                       ...      ...    ...       ...      ...       ...      ...      ...\n",
       "Dask Name: from-delayed, 444 tasks"
      ],
      "text/html": "<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Datetime</th>\n      <th>Level</th>\n      <th>Ask_Price</th>\n      <th>Ask_Size</th>\n      <th>Bid_Price</th>\n      <th>Bid_Size</th>\n      <th>Sequence</th>\n    </tr>\n    <tr>\n      <th>npartitions=148</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th></th>\n      <td>int64</td>\n      <td>object</td>\n      <td>int64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>float64</td>\n      <td>int64</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div>Dask Name: from-delayed, 444 tasks</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "%%time\n",
    "raw_data_path = f'./S3_data'\n",
    "caching_folder = f'{experiments_folder}/cache' \n",
    "cache_data = LOBData.LOBData(raw_data_path, pair, caching_folder, frequency = timedelta(seconds=10), levels=10)\n",
    "\n",
    "cache_data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_dyn_df, test_dyn_df, top_ob_train, top_ob_test = import_px_data(experiments_folder, frequency, pair, date_start, date_end, lob_depth, norm_type, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df.reset_index(), output_type='array') # 1 train dataset\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2, index=train_dt_index_dyn) # 2\n",
    "px_ts_train = top_ob_train.reset_index()[['Mid_Price']]\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df.reset_index(), output_type='array') # 1 test dataset\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2, index=test_dt_index_dyn) # 2\n",
    "px_ts_test = top_ob_test.reset_index()[['Mid_Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trades"
   ]
  },
  {
   "source": [
    "### Trades"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading trade files\n",
      "saving file at Experiments/input/USDT_BTC-trades-10s-2020_04_01-2020_12_31.csv.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       globalTradeID   tradeID                 date  type          rate  \\\n",
       "0          456641156  33111689  2020-04-01 00:00:59   buy   6420.617001   \n",
       "1          456641153  33111688  2020-04-01 00:00:58  sell   6420.505196   \n",
       "2          456641152  33111687  2020-04-01 00:00:56  sell   6419.109892   \n",
       "3          456641149  33111686  2020-04-01 00:00:56   buy   6420.938504   \n",
       "4          456641144  33111685  2020-04-01 00:00:54   buy   6420.505169   \n",
       "...              ...       ...                  ...   ...           ...   \n",
       "24302      505065984  39512256  2020-12-31 23:59:14   buy  28940.838661   \n",
       "24303      505065981  39512255  2020-12-31 23:59:12   buy  28927.499553   \n",
       "24304      505065968  39512254  2020-12-31 23:59:02   buy  28917.664350   \n",
       "24305      505065967  39512253  2020-12-31 23:59:00  sell  28917.145955   \n",
       "24306      505065963  39512252  2020-12-31 23:59:00   buy  28924.230430   \n",
       "\n",
       "         amount        total   orderNumber  \n",
       "0      0.000543     3.486202  519637736227  \n",
       "1      0.012052    77.377810  519637716247  \n",
       "2      0.100000   641.910989  519637697266  \n",
       "3      0.000543     3.486634  519637688275  \n",
       "4      0.022823   146.534676  519637663300  \n",
       "...         ...          ...           ...  \n",
       "24302  0.000090     2.604675  827670743878  \n",
       "24303  0.247920  7171.705689  827670700921  \n",
       "24304  0.155991  4510.895669  827670615007  \n",
       "24305  0.000115     3.332990  827670595027  \n",
       "24306  0.005722   165.510810  827670580042  \n",
       "\n",
       "[6400604 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>globalTradeID</th>\n      <th>tradeID</th>\n      <th>date</th>\n      <th>type</th>\n      <th>rate</th>\n      <th>amount</th>\n      <th>total</th>\n      <th>orderNumber</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>456641156</td>\n      <td>33111689</td>\n      <td>2020-04-01 00:00:59</td>\n      <td>buy</td>\n      <td>6420.617001</td>\n      <td>0.000543</td>\n      <td>3.486202</td>\n      <td>519637736227</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>456641153</td>\n      <td>33111688</td>\n      <td>2020-04-01 00:00:58</td>\n      <td>sell</td>\n      <td>6420.505196</td>\n      <td>0.012052</td>\n      <td>77.377810</td>\n      <td>519637716247</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>456641152</td>\n      <td>33111687</td>\n      <td>2020-04-01 00:00:56</td>\n      <td>sell</td>\n      <td>6419.109892</td>\n      <td>0.100000</td>\n      <td>641.910989</td>\n      <td>519637697266</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>456641149</td>\n      <td>33111686</td>\n      <td>2020-04-01 00:00:56</td>\n      <td>buy</td>\n      <td>6420.938504</td>\n      <td>0.000543</td>\n      <td>3.486634</td>\n      <td>519637688275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>456641144</td>\n      <td>33111685</td>\n      <td>2020-04-01 00:00:54</td>\n      <td>buy</td>\n      <td>6420.505169</td>\n      <td>0.022823</td>\n      <td>146.534676</td>\n      <td>519637663300</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24302</th>\n      <td>505065984</td>\n      <td>39512256</td>\n      <td>2020-12-31 23:59:14</td>\n      <td>buy</td>\n      <td>28940.838661</td>\n      <td>0.000090</td>\n      <td>2.604675</td>\n      <td>827670743878</td>\n    </tr>\n    <tr>\n      <th>24303</th>\n      <td>505065981</td>\n      <td>39512255</td>\n      <td>2020-12-31 23:59:12</td>\n      <td>buy</td>\n      <td>28927.499553</td>\n      <td>0.247920</td>\n      <td>7171.705689</td>\n      <td>827670700921</td>\n    </tr>\n    <tr>\n      <th>24304</th>\n      <td>505065968</td>\n      <td>39512254</td>\n      <td>2020-12-31 23:59:02</td>\n      <td>buy</td>\n      <td>28917.664350</td>\n      <td>0.155991</td>\n      <td>4510.895669</td>\n      <td>827670615007</td>\n    </tr>\n    <tr>\n      <th>24305</th>\n      <td>505065967</td>\n      <td>39512253</td>\n      <td>2020-12-31 23:59:00</td>\n      <td>sell</td>\n      <td>28917.145955</td>\n      <td>0.000115</td>\n      <td>3.332990</td>\n      <td>827670595027</td>\n    </tr>\n    <tr>\n      <th>24306</th>\n      <td>505065963</td>\n      <td>39512252</td>\n      <td>2020-12-31 23:59:00</td>\n      <td>buy</td>\n      <td>28924.230430</td>\n      <td>0.005722</td>\n      <td>165.510810</td>\n      <td>827670580042</td>\n    </tr>\n  </tbody>\n</table>\n<p>6400604 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "fetch_s3_trade_files('S3_data', 'Experiments/input', 'USDT_BTC', timedelta(seconds=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quotes = pd.read_csv(f'Experiments/input/USDT_BTC--10lev--10sec--2020_04_04--2021_01_03.csv.gz', index_col=0)\n",
    "# df_quotes['Datetime'] = pd.to_datetime(df_quotes['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_trades_folder = 'S3_data/trades'\n",
    "pair = 'USDT_BTC'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = pd.read_csv('Experiments/input/USDT_BTC-trades-10s-2020_04_01-2020_12_31.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades.groupby(pd.Grouper(key='date', freq='D')).agg({'total':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.82 s, sys: 488 ms, total: 4.31 s\nWall time: 4.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_trades['date'] = pd.to_datetime(df_trades['date'])\n",
    "df_trades_grp = df_trades.groupby([pd.Grouper(key='date', freq='10s', dropna=False), 'type']).agg({'amount':'sum', 'rate':'mean'}).reset_index()\n",
    "df_trades_piv = df_trades_grp.pivot(values=['amount', 'rate'], columns='type',index='date').reset_index()\n",
    "\n",
    "df_trades_piv.columns = list(map(\"_\".join, df_trades_piv.columns)) # \"flatten\" column names\n",
    "df_trades_piv.rename(columns={'date_':'Datetime', 'amount_buy':'Ask_Size', 'amount_sell':'Bid_Size', 'rate_buy':'Ask_Price', 'rate_sell':'Bid_Price'}, inplace=True)\n",
    "\n",
    "# fill gaps with no trades\n",
    "date_range_reindex = pd.DataFrame(pd.date_range(df_trades_piv['Datetime'].min(), df_trades_piv['Datetime'].max(), freq=\"10s\"), columns=['Datetime'])\n",
    "df_trades_piv = pd.merge(df_trades_piv, date_range_reindex, right_on='Datetime', left_on='Datetime', how='right')\n",
    "\n",
    "# impute NAs - zero for size and last px for price\n",
    "df_trades_piv.loc[:,['Ask_Size', 'Bid_Size']] = df_trades_piv.loc[:,['Ask_Size', 'Bid_Size']].fillna(0)\n",
    "df_trades_piv.loc[:,['Ask_Price', 'Bid_Price']] = df_trades_piv.loc[:,['Ask_Price', 'Bid_Price']].fillna(method='ffill')\n",
    "\n",
    "\n",
    "df_trades_piv['Level'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 4.84 s, sys: 1.19 s, total: 6.04 s\nWall time: 6.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_all = pd.concat([df_trades_piv, df_quotes]).sort_values(by=['Datetime', 'Level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stdz = df_all[(df_all['Datetime']>='2020-01-01 00:00:00')&(df_all['Datetime']<='2020-12-31 23:59:50')]\n",
    "df_stdz.set_index(['Datetime','Level'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rolling window = 950400, calculate as roll: 43200 * levels: 11 * shape[1]: 2\n",
      "done\n",
      "rolling window = 950400, calculate as roll: 43200 * levels: 11 * shape[1]: 2\n",
      "done\n",
      "CPU times: user 46.3 s, sys: 10.6 s, total: 56.9 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "norm_df_size = standardize(df_stdz[['Ask_Size', 'Bid_Size']], 10+1, 'dyn_z_score', roll)\n",
    "norm_df_price = standardize(df_stdz[['Ask_Price', 'Bid_Price']], 10+1, 'dyn_z_score', roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           Ask_Size      Bid_Size     Ask_Price     Bid_Price  Unnamed: 0.1  \\\n",
       "count  2.596320e+07  2.596320e+07  2.596320e+07  2.596320e+07  2.358720e+07   \n",
       "mean   7.529761e-01  6.687193e-01  1.203929e+04  1.202854e+04  4.319950e+04   \n",
       "std    1.284103e+00  1.254996e+00  4.452705e+03  4.444519e+03  2.494153e+04   \n",
       "min    0.000000e+00  0.000000e+00  6.153252e+03  6.152474e+03  0.000000e+00   \n",
       "25%    4.268000e-02  3.382259e-02  9.275622e+03  9.270314e+03  2.159975e+04   \n",
       "50%    2.000000e-01  1.659653e-01  1.067745e+04  1.067097e+04  4.319950e+04   \n",
       "75%    9.782285e-01  7.265500e-01  1.299600e+04  1.298473e+04  6.479925e+04   \n",
       "max    1.125158e+02  1.347703e+02  2.944634e+04  2.930673e+04  8.639900e+04   \n",
       "\n",
       "           Sequence  \n",
       "count  2.358720e+07  \n",
       "mean   9.505883e+08  \n",
       "std    1.180569e+08  \n",
       "min    7.138388e+08  \n",
       "25%    8.634625e+08  \n",
       "50%    9.581165e+08  \n",
       "75%    1.049790e+09  \n",
       "max    1.143096e+09  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ask_Size</th>\n      <th>Bid_Size</th>\n      <th>Ask_Price</th>\n      <th>Bid_Price</th>\n      <th>Unnamed: 0.1</th>\n      <th>Sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.596320e+07</td>\n      <td>2.596320e+07</td>\n      <td>2.596320e+07</td>\n      <td>2.596320e+07</td>\n      <td>2.358720e+07</td>\n      <td>2.358720e+07</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.529761e-01</td>\n      <td>6.687193e-01</td>\n      <td>1.203929e+04</td>\n      <td>1.202854e+04</td>\n      <td>4.319950e+04</td>\n      <td>9.505883e+08</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.284103e+00</td>\n      <td>1.254996e+00</td>\n      <td>4.452705e+03</td>\n      <td>4.444519e+03</td>\n      <td>2.494153e+04</td>\n      <td>1.180569e+08</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>6.153252e+03</td>\n      <td>6.152474e+03</td>\n      <td>0.000000e+00</td>\n      <td>7.138388e+08</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.268000e-02</td>\n      <td>3.382259e-02</td>\n      <td>9.275622e+03</td>\n      <td>9.270314e+03</td>\n      <td>2.159975e+04</td>\n      <td>8.634625e+08</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.000000e-01</td>\n      <td>1.659653e-01</td>\n      <td>1.067745e+04</td>\n      <td>1.067097e+04</td>\n      <td>4.319950e+04</td>\n      <td>9.581165e+08</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.782285e-01</td>\n      <td>7.265500e-01</td>\n      <td>1.299600e+04</td>\n      <td>1.298473e+04</td>\n      <td>6.479925e+04</td>\n      <td>1.049790e+09</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.125158e+02</td>\n      <td>1.347703e+02</td>\n      <td>2.944634e+04</td>\n      <td>2.930673e+04</td>\n      <td>8.639900e+04</td>\n      <td>1.143096e+09</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "df_stdz.describe()"
   ]
  },
  {
   "source": [
    "## Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train labels\n",
    "mid_px_train = px_ts_train['Mid_Price']\n",
    "labels_train, smoothed_px_train, df_trades_train = cleaned_labels(mid_px_train, method=label_technique, print_details=False)\n",
    "\n",
    "# encode\n",
    "encoded_train_labels = np_utils.to_categorical(labels_train.values,3) \n",
    "\n",
    "# # get transaction df\n",
    "# strategy_df_train = get_strategy_pnl(mid_px_train, labels_train)\n",
    "\n",
    "viz_t.plot_labels_line(mid_px_train[start_plot:end_plot], \n",
    "    labels_train[start_plot:end_plot], \n",
    "    title='Train Labels', \n",
    "    smoothed_signal=smoothed_px_train[start_plot:end_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test labels\n",
    "mid_px_test = px_ts_test['Mid_Price']\n",
    "labels_test, smoothed_px_test, df_trades_test = cleaned_labels(mid_px_test, method=label_technique, print_details=False)\n",
    "\n",
    "# encode\n",
    "encoded_test_labels = np_utils.to_categorical(labels_test.values,3) \n",
    "\n",
    "# # get transaction df\n",
    "# strategy_df_test = get_strategy_pnl(mid_px_test, labels_test)\n",
    "\n",
    "viz_t.plot_labels_line(mid_px_test[start_plot:end_plot], \n",
    "    labels_test[start_plot:end_plot], \n",
    "    title='Test Labels', \n",
    "    smoothed_signal=smoothed_px_test[start_plot:end_plot])"
   ]
  },
  {
   "source": [
    "## Visual checks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_t.plot_trades_distribution(df_trades_train[df_trades_train['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viz_t.plot_trades_length_overview(df_trades_train[df_trades_train['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.histogram()\n",
    "# fig.add_trace(go.Histogram(x=df_trades_train['trade_len'].values, name='train', autobinx = False, xbins={'size':5}))\n",
    "# fig.add_trace(go.Histogram(x=df_trades_test['trade_len'].values, name='test', autobinx = False, xbins={'size':5}))\n",
    "\n",
    "# # The two histograms are drawn on top of another\n",
    "# fig.update_layout(barmode='overlay')\n",
    "# fig.update_traces(opacity=0.75)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_timeseries(ts_list=[top_ob_train.set_index('Datetime')['Mid_Price'], top_ob_test.set_index('Datetime')['Mid_Price'], mid_px_train_dyn, mid_px_test_dyn], primary_axis=[True, True, False, False], legend=['train-px', 'test-px', 'train-dyn', 'test-dyn'], sample_size=180)"
   ]
  },
  {
   "source": [
    "## Model Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_deeplob(T, lob_depth):\n",
    "    ## just a test\n",
    "\n",
    "    input_lmd = Input(shape=(T, lob_depth * 4, 1))\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    \n",
    "    # note on learnable parameters: CONV2(filter shape =1*2, stride=1) layer is: ((shape of width of filter * shape of height filter * number of filters in the previous layer+1) * number of filters) = 2080 or ((2*1*32)+1)*32\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(16, (1, lob_depth))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    print(conv_first1.shape)\n",
    "\n",
    "    convfirst_output = Reshape((int(conv_first1.shape[1])* int(conv_first1.shape[3]),))(conv_first1)\n",
    "    print(convfirst_output.shape)\n",
    "    # note on learnable parameters:FC3 layer is((current layer c*previous layer p)+1*c) with c being number of neurons\n",
    "    out = Dense(3, activation='softmax')(convfirst_output)\n",
    "    print(out.shape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_code = inspect.getsource(create_light_deeplob)\n",
    "lines_with_short_desription = [line for line in model_code.split('\\n') if \"##\" in line]\n",
    "short_description = re.sub(r'\\W+', '_', lines_with_short_desription[0])\n",
    "\n",
    "create_light_deeplob(length, lob_depth).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment_id = f'{date_time_now}-{pair}-{frequency.seconds}s-{lob_depth}l-{length}-{date_start}-{date_end}{short_description}'\n",
    "results_folder = f'{experiments_folder}/{pair}/{experiment_id}'\n",
    "os.makedirs(f'{results_folder}', exist_ok=True)\n",
    "batch_size=256\n",
    "\n",
    "config = {\n",
    "  'pair': pair,\n",
    "  'frequency': frequency.seconds,\n",
    "  'lob_depth': lob_depth,\n",
    "  'length': length,\n",
    "  'date_start': date_start,\n",
    "  'date_end': date_end,\n",
    "  'norm_type': norm_type,\n",
    "  'roll': roll,\n",
    "  'batch_size': batch_size,\n",
    "  'label_technique': label_technique\n",
    "#   'min_profit': min_profit,\n",
    "#   'k_plus': k_plus,\n",
    "#   'k_minus': k_minus,\n",
    "#   'alpha': alpha,\n",
    "#   'trading_fee': trading_fee,\n",
    "\n",
    "#   'input': input_file_name,\n",
    "#   'normalized_train_file': normalized_train_file,\n",
    "#   'normalized_test_file':   normalized_test_file,\n",
    "#   'top_ob_train_file': top_ob_train_file,\n",
    "#   'top_ob_test_file': top_ob_test_file\n",
    "}\n",
    "\n",
    "with open(f'{results_folder}/config.json', 'w') as fp:\n",
    "    json.dump(config, fp, default=str)\n",
    "\n",
    "with open(f'{results_folder}/model_code.py', 'w') as fp:\n",
    "    fp.write(model_code)\n",
    "\n",
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "with open(f'{results_folder}/model_summary.txt', 'w') as fp:\n",
    "    light_deeplob.summary(print_fn=lambda x: fp.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to train the model on smoother version of the data"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "\n",
    "model_checkpoint_path = f'{results_folder}/{experiment_id}.h5'\n",
    "\n",
    "# Learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                   factor=0.2, \n",
    "                                                   patience=20)\n",
    "\n",
    "# Checkpoint callback. Saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=2,\n",
    "                                                 save_freq='epoch') # every epoch\n",
    "\n",
    "# Early stopping callback. When sees no progress on the validation set\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "# Tensorboard callback\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(results_folder)\n",
    "\n",
    "# Train and Test time series generators\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth_dyn,\n",
    "    encoded_train_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# to be replaced with validation?\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_.h5'\n",
    "# loaded_light_deep_lob = tf.keras.models.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "light_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=0,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights and evaluate model performance\n",
    "deep_lob_loaded = tf.keras.models.load_model(model_name)\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def evaluate_model(model):\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(generator_test, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "\n",
    "#evaluate_model(deep_lob_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels\n",
    "predictions_prob = deep_lob_loaded.predict(generator_test, verbose=1)\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels = pd.Series(map_labels(np.argmax(predictions_prob,axis=1)), name='predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental: predicted labels on rolling avg\n",
    "predictions_prob_wa = pd.DataFrame(predictions_prob).rolling(window=10).mean().values\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels_wa = pd.Series(map_labels(np.argmax(predictions_prob_wa,axis=1)), name='predicted_labels_wa') # back to original 1,0,-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Predicted labels #####')\n",
    "label_insights(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Weighted average predicted labels #####')\n",
    "label_insights(predicted_labels_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.shape, test_depth_dyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dangerous assigning offset here, wrap it into a function\n",
    "# align prediction with \"length\" offset\n",
    "index_range = np.arange(length, predicted_labels.shape[0] + length) # offset ts length fed to ts generator\n",
    "predicted_labels.index = index_range\n",
    "\n",
    "# generate timeseries with buy, sell, zero prob\n",
    "buy_prob = pd.Series(predictions_prob[:,1], index=index_range)\n",
    "sell_prob = pd.Series(predictions_prob[:,2], index=index_range)\n",
    "zero_prob = pd.Series(predictions_prob[:,0], index=index_range)\n",
    "\n",
    "buy_prob_wa = pd.Series(predictions_prob_wa[:,1], index=index_range)\n",
    "\n",
    "viz_t.plot_labels_line(top_ob_test['Mid_Price'][start_plot:end_plot], \n",
    "    labels_test[start_plot:end_plot], # original labels\n",
    "    title='Train Set Labels', \n",
    "    #smoothed_signal=smoothed_px_test[start_plot:end_plot],\n",
    "    predicted_labels=predicted_labels[start_plot:end_plot],\n",
    "    buy_prob_labels=buy_prob[start_plot:end_plot],\n",
    "    #sell_prob_labels=sell_prob[start:end],\n",
    "    predictions_prob_wa=buy_prob_wa[start_plot:end_plot],\n",
    "    width=1100, height=600\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOUBLE CHECK that labels and px_ts are correctly aligned\n",
    "px_ts = top_ob_test['Mid_Price']# adjust prediction offsset\n",
    "datetime_ts = top_ob_test['Datetime']\n",
    "trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px_ts = top_ob_test['Mid_Price'][length:].reset_index()['Mid_Price']# adjust prediction offsset\n",
    "# datetime_ts = top_ob_test['Datetime'][length:].reset_index()['Datetime']\n",
    "# trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "# df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to do:\n",
    "# seek for patterns in prediction probability\n",
    "# day vs night - weekday vs weekend - model certainty before long trades vs short trades\n",
    "# determine if predictions are naive"
   ]
  }
 ]
}