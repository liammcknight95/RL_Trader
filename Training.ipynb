{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python (rltrader_env)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from func_tools import standardize, cnn_data_reshaping, reshape_lob_levels, label_insights, back_to_labels, get_strategy_pnl,intraday_vol_ret \n",
    "\n",
    "import visualization_tools as viz_t\n",
    "\n",
    "from labelling_class import Labels_Generator\n",
    "\n",
    "#import multiprocessing\n",
    "#import glob\n",
    "import inspect\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, BatchNormalization, LeakyReLU, concatenate, add, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict tf to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_folder = 'Experiments'\n",
    "input_data_folder = f'{experiments_folder}/input'\n",
    "cache_folder = f'{experiments_folder}/cache'\n",
    "\n",
    "pair = 'USDT_BTC'\n",
    "frequency = timedelta(seconds=10)\n",
    "lob_depth = 10\n",
    "length = 100\n",
    "date_start = '2020_04_04'\n",
    "date_end = '2021_01_03'\n",
    "norm_type = 'dyn_z_score'\n",
    "roll = 7200 * 6\n",
    "batch_size = 64\n",
    "\n",
    "# labelling inputs\n",
    "k_plus = 15\n",
    "k_minus = 15\n",
    "alpha = 0.0045\n",
    "trading_fee=0.000712\n",
    "min_profit=0.002\n",
    "\n",
    "frequency_seconds = int(frequency.total_seconds())\n",
    "os.makedirs(f'{cache_folder}/{pair}', exist_ok=True)\n",
    "\n",
    "# Data import - needs to be adjusted importing from several files using Dask\n",
    "input_file_name = f'{pair}--{lob_depth}lev--{frequency_seconds}sec--{date_start}--{date_end}.csv.gz'\n",
    "\n",
    "normalized_train_file = f'{cache_folder}/{pair}/TRAIN--{norm_type}-{roll}--{input_file_name}'\n",
    "normalized_test_file = f'{cache_folder}/{pair}/TEST--{norm_type}-{roll}--{input_file_name}'\n",
    "\n",
    "top_ob_train_file = f'{cache_folder}/{pair}/TRAIN_TOP--{input_file_name}'\n",
    "top_ob_test_file = f'{cache_folder}/{pair}/TEST_TOP--{input_file_name}'\n",
    "\n",
    "if os.path.isfile(normalized_test_file): # testing for one of cache files, assuming all were saved \n",
    "  print(f'Reading cached {normalized_train_file}')\n",
    "  train_dyn_df = pd.read_csv(normalized_train_file)\n",
    "  print(f'Reading cached {normalized_test_file}')\n",
    "  test_dyn_df = pd.read_csv(normalized_test_file)\n",
    "\n",
    "  print(f'Reading cached {top_ob_train_file}')\n",
    "  top_ob_train = pd.read_csv(top_ob_train_file)\n",
    "  print(f'Reading cached {top_ob_test_file}')\n",
    "  top_ob_test = pd.read_csv(top_ob_test_file)  \n",
    "\n",
    "else:\n",
    "  print(f'Reading {input_data_folder}/{input_file_name}')\n",
    "  data = pd.read_csv(f'{input_data_folder}/{input_file_name}', index_col=0)\n",
    "  assert lob_depth == data['Level'].max() + 1 # number of levels of order book\n",
    "\n",
    "  # Train test split\n",
    "  train_test_split = int((data.shape[0] / lob_depth) * 0.7) # slice reference for train and test\n",
    "  train_timestamps = data['Datetime'].unique()[:train_test_split]\n",
    "  test_timestamps = data['Datetime'].unique()[train_test_split:]\n",
    "\n",
    "  train_cached_data = data[data['Datetime'].isin(train_timestamps)].set_index(['Datetime', 'Level'])\n",
    "  test_cached_data = data[data['Datetime'].isin(test_timestamps)].set_index(['Datetime', 'Level'])\n",
    "\n",
    "  print(f'Train dataset shape: {train_cached_data.shape} - Test dataset shape: {test_cached_data.shape}')\n",
    "\n",
    "  roll_shift = roll+1 # rolling period for dyn z score - + 1 from shift in ft.normalize\n",
    "\n",
    "  train_dyn_prices = normalize(train_cached_data[['Ask_Price', 'Bid_Price']], lob_depth, 'dyn_z_score', roll)\n",
    "  train_dyn_volumes = normalize(train_cached_data[['Ask_Size', 'Bid_Size']], lob_depth, 'dyn_z_score', roll)\n",
    "  train_dyn_df = pd.concat([train_dyn_prices, train_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "  print(f'Saving {normalized_train_file}')\n",
    "  train_dyn_df.to_csv(normalized_train_file, compression='gzip') # save normalized data to csv \n",
    "\n",
    "  top_ob_train = train_cached_data[train_cached_data.index.get_level_values(1)==0][roll_shift:]\n",
    "  top_ob_train['Mid_Price'] = (top_ob_train['Ask_Price'] + top_ob_train['Bid_Price']) / 2\n",
    "  top_ob_train['Spread'] = (top_ob_train['Ask_Price'] - top_ob_train['Bid_Price']) / top_ob_train['Mid_Price']\n",
    "  top_ob_train['merge_index'] = top_ob_train.reset_index().index.values # useful for merging later\n",
    "  print(f'Saving {top_ob_train_file}')\n",
    "  top_ob_train.to_csv(top_ob_train_file, compression='gzip') # save top level not normalized to csv\n",
    "\n",
    "  # print(f'Saving {normalized_data_folder}/{pair}/TRAIN_top--{norm_type}-{roll}--{input_file_name}')\n",
    "  # train_dyn_df[train_dyn_df['Level']==0].to_csv(f'{normalized_data_folder}/{pair}/TRAIN_TOP--{norm_type}-{roll}--{input_file_name}', compression='gzip') # save top level to csv \n",
    "\n",
    "  test_dyn_prices = normalize(test_cached_data[['Ask_Price', 'Bid_Price']], lob_depth, 'dyn_z_score', roll)\n",
    "  test_dyn_volumes = normalize(test_cached_data[['Ask_Size', 'Bid_Size']], lob_depth, 'dyn_z_score', roll)\n",
    "  test_dyn_df = pd.concat([test_dyn_prices, test_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "  print(f'Saving {normalized_test_file}')\n",
    "  test_dyn_df.to_csv(normalized_test_file, compression='gzip') # save normalized data to csv\n",
    "\n",
    "  top_ob_test = test_cached_data[test_cached_data.index.get_level_values(1)==0][roll_shift:]\n",
    "  top_ob_test['Mid_Price'] = (top_ob_test['Ask_Price'] + top_ob_test['Bid_Price']) / 2\n",
    "  top_ob_test['Spread'] = (top_ob_test['Ask_Price'] - top_ob_test['Bid_Price']) / top_ob_test['Mid_Price']\n",
    "  top_ob_test['merge_index'] = top_ob_test.reset_index().index.values # useful for merging later\n",
    "  print(f'Saving {top_ob_test_file}')\n",
    "  top_ob_test.to_csv(top_ob_test_file, compression='gzip') # # save top level not normalized to csv\n",
    "\n",
    "  # print(f'Saving {normalized_data_folder}/{pair}/TEST_TOP--{norm_type}-{roll}--{input_file_name}')\n",
    "  # test_dyn_df[test_dyn_df['Level']==0].to_csv(f'{normalized_data_folder}/{pair}/TEST_TOP--{norm_type}-{roll}--{input_file_name}', compression='gzip') # save top level to csv \n",
    "\n",
    "display(train_dyn_df.describe()) # check train data overview\n",
    "display(test_dyn_df.describe()) # check test data overview\n",
    "\n",
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df, output_type='array') # 1 train dataset\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2) # 2\n",
    "px_ts_train = top_ob_train.reset_index()[['Mid_Price']]\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df, output_type='array') # 1 test dataset\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2) # 2\n",
    "px_ts_test = top_ob_test.reset_index()[['Mid_Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(top_ob_test.index.date).unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cached_data.shape, top_ob_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# labels = get_labels(top_ob_train.reset_index()['Mid_Price'], int(k_plus), int(k_minus), alpha, long_only=False)\n",
    "# profit = get_strategy_pnl(top_ob_train.reset_index()['Mid_Price'], labels, trading_fee=0.000712, min_profit=0.0020, plotting=False, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to know opt profit for these labels"
   ]
  },
  {
   "source": [
    "## Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Train Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant to add to avoid negative value (problems with log rets)\n",
    "# const = -min(mid_px_train_dyn.min(), mid_px_test_dyn.min())  + 0.1\n",
    "# mid_px_train_dyn_shifted = mid_px_train_dyn.rename('mid_px_dyn')\n",
    "# mid_px_train_dyn_shifted = mid_px_train_dyn_shifted + const\n",
    "mid_px_train = px_ts_train['Mid_Price']\n",
    "# train labels\n",
    "train_labels_gen = Labels_Generator(mid_px_train)\n",
    "\n",
    "#step 1\n",
    "print('\\n##### Step 1 #####')\n",
    "train_labels_gen.get_raw_labels()\n",
    "label_insights(train_labels_gen.labels)\n",
    "\n",
    "\n",
    "# step 2 - first cleaning\n",
    "print('\\n##### Step 2 #####')\n",
    "df_trades2 = train_labels_gen.get_cleaned_labels(fillna_method='ffill', gross_returns=0.005, trade_len=20)\n",
    "label_insights(train_labels_gen.labels)\n",
    "\n",
    "# step 3 - second cleaning\n",
    "print('\\n##### Step 3 #####')\n",
    "df_trades3 = train_labels_gen.get_cleaned_labels(fillna_value=0, gross_returns=0.005, trade_len=30)#, gross_returns=0.002)\n",
    "label_insights(train_labels_gen.labels)\n",
    "viz_t.plot_labels_line(mid_px_train[start:end], \n",
    "    train_labels_gen.labels[start:end], \n",
    "    title='Train Labels', \n",
    "    smoothed_signal=train_labels_gen.get_smooth_px()[start:end])\n",
    "\n",
    "labels_train = train_labels_gen.labels\n",
    "\n",
    "# get transaction df\n",
    "strategy_df_train = get_strategy_pnl(mid_px_train, labels_train)\n",
    "\n",
    "# encode\n",
    "encoded_train_labels = np_utils.to_categorical(labels_train.values,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_distribution(df_trades3[df_trades3['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_length_overview(df_trades3[df_trades3['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "source": [
    "#### Test labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant to add to avoid negative value (problems with log rets)\n",
    "# mid_px_test_dyn_shifted = mid_px_test_dyn.rename('mid_px_dyn')\n",
    "# mid_px_test_dyn_shifted = mid_px_test_dyn_shifted + const\n",
    "mid_px_test = px_ts_test['Mid_Price']\n",
    "# test labels\n",
    "test_labels_gen = Labels_Generator(mid_px_test)\n",
    "\n",
    "#step 1\n",
    "print('\\n##### Step 1 #####')\n",
    "test_labels_gen.get_raw_labels()\n",
    "label_insights(test_labels_gen.labels)\n",
    "\n",
    "# step 2 - first cleaning\n",
    "print('\\n##### Step 2 #####')\n",
    "df_trades2 = test_labels_gen.get_cleaned_labels(fillna_method='ffill', gross_returns=0.005, trade_len=20)\n",
    "label_insights(test_labels_gen.labels)\n",
    "\n",
    "# step 3 - second cleaning\n",
    "print('\\n##### Step 3 #####')\n",
    "df_trades3 = test_labels_gen.get_cleaned_labels(fillna_value=0, gross_returns=0.005, trade_len=30)#, gross_returns=0.002)\n",
    "label_insights(test_labels_gen.labels)\n",
    "viz_t.plot_labels_line(mid_px_test[start:end], \n",
    "    test_labels_gen.labels[start:end], \n",
    "    title='test Labels', \n",
    "    smoothed_signal=test_labels_gen.get_smooth_px()[start:end])\n",
    "\n",
    "labels_test = test_labels_gen.labels\n",
    "\n",
    "# get transaction df\n",
    "strategy_df_test = get_strategy_pnl(mid_px_test, labels_test)\n",
    "\n",
    "# encode\n",
    "encoded_test_labels = np_utils.to_categorical(labels_test.values,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades3[df_trades3['cleaned_labels']!=0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_distribution(df_trades3[df_trades3['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_length_overview(df_trades3[df_trades3['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_test = strategy_df_test.dropna(subset=['gross_returns'])\n",
    "trades_test.groupby('labels')['trade_len'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_train = strategy_df_train.dropna(subset=['gross_returns'])\n",
    "trades_train.groupby('labels')['trade_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram()\n",
    "fig.add_trace(go.Histogram(x=trades_train['trade_len'].values, name='train', autobinx = False, xbins={'size':5}))\n",
    "fig.add_trace(go.Histogram(x=trades_test['trade_len'].values, name='test', autobinx = False, xbins={'size':5}))\n",
    "\n",
    "# The two histograms are drawn on top of another\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "## Visual check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLED MID PX CHART - create a func tool function for this\n",
    "sample_size = 6 * 5#6*60*24 # daily\n",
    "dynz_gap = int(roll / sample_size)\n",
    "hourly_mid_line = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "y_train = top_ob_train['Mid_Price'].iloc[::sample_size].values\n",
    "x_train = np.arange(y_train.shape[0])\n",
    "y_test = top_ob_test['Mid_Price'].iloc[::sample_size].values\n",
    "x_test = np.arange(y_train.shape[0] + dynz_gap, y_train.shape[0] + y_test.shape[0] + dynz_gap)\n",
    "\n",
    "y_train_dynz = mid_px_train_dyn.iloc[::sample_size].values  \n",
    "x_train_dynz = np.arange(y_train.shape[0])\n",
    "y_test_dynz = mid_px_test_dyn.iloc[::sample_size].values\n",
    "x_test_dynz = np.arange(y_train.shape[0] + dynz_gap, y_train.shape[0] + y_test.shape[0] + dynz_gap)\n",
    "\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_train, x=x_train, name='mid_train'), secondary_y=False)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_test, x=x_test, name='mid_test'), secondary_y=False)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_train_dynz, x=x_train_dynz, name='mid_train_dynz',\n",
    "    marker=dict(color='rgba(44, 130, 201, 0.3)')), secondary_y=True)\n",
    "hourly_mid_line.add_trace(go.Scatter(y=y_test_dynz, x=x_test_dynz, name='mid_test_dynz',\n",
    "    marker=dict(color='rgba(240, 52, 52, 0.3)')), secondary_y=True)\n",
    "\n",
    "hourly_mid_line.update_yaxes(fixedrange= True, secondary_y=True)\n",
    "\n",
    "hourly_mid_line.update_layout(title='<b>Sampled mid</b>')\n",
    "hourly_mid_line.show()"
   ]
  },
  {
   "source": [
    "## Model Training & Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_deeplob(T, lob_depth):\n",
    "    ## big lr, big batch size 16 filter size, shuffle\n",
    "\n",
    "    input_lmd = Input(shape=(T, lob_depth * 4, 1))\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (1, 20), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    \n",
    "    # note on learnable parameters: CONV2(filter shape =1*2, stride=1) layer is: ((shape of width of filter * shape of height filter * number of filters in the previous layer+1) * number of filters) = 2080 or ((2*1*32)+1)*32\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(16, (1, lob_depth))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    # conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    # conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    print(conv_first1.shape)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "\n",
    "            \n",
    "    # # build the inception module\n",
    "    # convsecond_1 = Conv2D(32, (1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    # convsecond_1 = Conv2D(32, (3, 1), padding='same')(convsecond_1)\n",
    "    # convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    # convsecond_1 = BatchNormalization()(convsecond_1)\n",
    "    # # convsecond_1 = Dropout(.5)(convsecond_1)\n",
    "\n",
    "    # convsecond_2 = Conv2D(32, (1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    # convsecond_2 = Conv2D(32, (5, 1), padding='same')(convsecond_2)\n",
    "    # convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    \n",
    "    # convsecond_2 = BatchNormalization()(convsecond_2)\n",
    "    # convsecond_2 = Dropout(.5)(convsecond_2)\n",
    "    # convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    # convsecond_3 = Conv2D(32, (1, 1), padding='same')(convsecond_3)\n",
    "    # convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    # convsecond_3 = BatchNormalization()(convsecond_3)\n",
    "    # convsecond_3 = Dropout(.5)(convsecond_3)\n",
    "    \n",
    "    # convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3) #, convsecond_3, convsecond_4\n",
    "    # print(convsecond_output.shape)\n",
    "\n",
    "    # # use the MC dropout here\n",
    "    # conv_reshape = Reshape((int(convsecond_output.shape[1])* int(convsecond_output.shape[3]),))(convsecond_output)\n",
    "    # print(conv_reshape)\n",
    "    convfirst_output = Reshape((int(conv_first1.shape[1])* int(conv_first1.shape[3]),))(conv_first1)\n",
    "    print(convfirst_output.shape)\n",
    "    # note on learnable parameters:FC3 layer is((current layer c*previous layer p)+1*c) with c being number of neurons\n",
    "    out = Dense(3, activation='softmax')(convfirst_output)\n",
    "    print(out.shape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_code = inspect.getsource(create_light_deeplob)\n",
    "lines_with_short_desription = [line for line in model_code.split('\\n') if \"##\" in line]\n",
    "short_description = re.sub(r'\\W+', '_', lines_with_short_desription[0])\n",
    "\n",
    "create_light_deeplob(length, lob_depth).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment_id = f'{date_time_now}-{pair}-{frequency_seconds}s-{lob_depth}l-{length}-{date_start}-{date_end}{short_description}'\n",
    "experiment_folder = f'{experiments_folder}/{pair}/{experiment_id}'\n",
    "os.makedirs(f'{experiment_folder}', exist_ok=True)\n",
    "batch_size=256\n",
    "\n",
    "config = {\n",
    "  'pair': pair,\n",
    "  'frequency': frequency_seconds,\n",
    "  'lob_depth': lob_depth,\n",
    "  'length': length,\n",
    "  'date_start': date_start,\n",
    "  'date_end': date_end,\n",
    "  'norm_type': norm_type,\n",
    "  'roll': roll,\n",
    "  'k_plus': k_plus,\n",
    "  'k_minus': k_minus,\n",
    "  'alpha': alpha,\n",
    "  'trading_fee': trading_fee,\n",
    "  'min_profit': min_profit,\n",
    "  'batch_size': batch_size,\n",
    "  'input': input_file_name,\n",
    "  'normalized_train_file': normalized_train_file,\n",
    "  'normalized_test_file':   normalized_test_file,\n",
    "  'top_ob_train_file': top_ob_train_file,\n",
    "  'top_ob_test_file': top_ob_test_file\n",
    "}\n",
    "\n",
    "with open(f'{experiment_folder}/config.json', 'w') as fp:\n",
    "    json.dump(config, fp, default=str)\n",
    "\n",
    "with open(f'{experiment_folder}/model_code.py', 'w') as fp:\n",
    "    fp.write(model_code)\n",
    "\n",
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "with open(f'{experiment_folder}/model_summary.txt', 'w') as fp:\n",
    "    light_deeplob.summary(print_fn=lambda x: fp.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to train the model on smoother version of the data"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "\n",
    "model_checkpoint_path = f'{experiment_folder}/{experiment_id}.h5'\n",
    "\n",
    "# Learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                   factor=0.2, \n",
    "                                                   patience=20)\n",
    "\n",
    "# Checkpoint callback. Saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=2,\n",
    "                                                 save_freq='epoch') # every epoch\n",
    "\n",
    "# Early stopping callback. When sees no progress on the validation set\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "# Tensorboard callback\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(experiment_folder)\n",
    "\n",
    "# Train and Test time series generators\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth_dyn,\n",
    "    encoded_train_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# to be replaced with validation?\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_.h5'\n",
    "# loaded_light_deep_lob = tf.keras.models.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "light_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=0,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])"
   ]
  },
  {
   "source": [
    "### Model results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights and evaluate model performance\n",
    "deep_lob_loaded = tf.keras.models.load_model(model_name)\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def evaluate_model(model):\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(generator_test, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model(deep_lob_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels\n",
    "predictions_prob = deep_lob_loaded.predict(generator_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_prob_wa = pd.DataFrame(predictions_prob).rolling(window=10).mean().values\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels_wa = pd.Series(map_labels(np.argmax(predictions_prob_wa,axis=1)), name='predicted_labels_wa') # back to original 1,0,-1\n",
    "predicted_labels = pd.Series(map_labels(np.argmax(predictions_prob,axis=1)), name='predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Predicted labels #####')\n",
    "label_insights(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Weighted average predicted labels #####')\n",
    "label_insights(predicted_labels_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.shape, test_depth_dyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dangerous assigning offset here, wrap it into a function\n",
    "offset=100 # offset for plotting\n",
    "start=0\n",
    "end=10000\n",
    "# align prediction offset\n",
    "index_range = np.arange(offset, predicted_labels.shape[0] + offset)\n",
    "predicted_labels.index = index_range\n",
    "buy_prob = pd.Series(predictions_prob[:,1], index=index_range)\n",
    "sell_prob = pd.Series(predictions_prob[:,2], index=index_range)\n",
    "zero_prob = pd.Series(predictions_prob[:,0], index=index_range)\n",
    "\n",
    "buy_prob_wa = pd.Series(predictions_prob_wa[:,1], index=index_range)\n",
    "\n",
    "plot_labels_line(top_ob_test['Mid_Price'][start:end], \n",
    "    test_labels_gen.labels[start:end], \n",
    "    title='Train Labels', \n",
    "    #smoothed_signal=test_labels_gen.get_smooth_px()[start:end],\n",
    "    predicted_labels=predicted_labels[start:end],\n",
    "    buy_prob_labels=buy_prob[start:end],\n",
    "    predictions_prob_wa=buy_prob_wa[start:end],\n",
    "    #sell_prob_labels=sell_prob[start:end],\n",
    "    #dun_px_label=(mid_px_test_dyn_shifted[start:end] - mid_px_test_dyn_shifted.mean())/mid_px_test_dyn_shifted.std()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = go.Figure(data=go.Scatter(x=buy_prob.index, y=buy_prob.values))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ts, vol_ts = intraday_vol_ret(mid, span=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_ts[10000:55000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ts[10000:55000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_ob_test.index = pd.to_datetime(top_ob_test['Datetime'])\n",
    "\n",
    "# mid = top_ob_test['Mid_Price']\n",
    "# mid = mid[:100000]\n",
    "# smooth_mid = Labels_Generator(mid).get_smooth_px()\n",
    "\n",
    "# smooth_mid.index = output.index\n",
    "# smooth_mid.name = 'Smoothed_mid'\n",
    "\n",
    "# import labelling_class\n",
    "# labelling_class.three_barrier_labelling(smooth_mid, h=700, factor=[1.0020, 0.9980])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_ts = top_ob_test['Mid_Price'][100:].reset_index()['Mid_Price']# adjust prediction offsset\n",
    "datetime_ts = top_ob_test['Datetime'][100:].reset_index()['Datetime']\n",
    "trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(px_ts, predicted_labels, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_prob = pd.Series(predictions_prob[:,1], name='buy_prob')\n",
    "sell_prob = pd.Series(predictions_prob[:,2], name='sell_prob')\n",
    "zero_prob = pd.Series(predictions_prob[:,0], name='zero_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_ts#top_ob_test[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to do:\n",
    "# need a sliding window to calculate rolling volatity - not sure about using rolling\n",
    "# seek for patterns in prediction probability\n",
    "# day vs night - weekday vs weekend - model certainty before long trades vs short trades\n",
    "# plot original labels and compare visually (could be part of db)\n",
    "# determine if predictions are naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand with other components of the order book\n",
    "timeseries_results = pd.concat([datetime_ts, trades_timeseries, buy_prob, sell_prob, zero_prob], axis=1)\n",
    "timeseries_results['10min_std'] = timeseries_results['log_ret'].rolling(6*10).std()\n",
    "timeseries_results['1hr_std'] = timeseries_results['log_ret'].rolling(6*60).std()\n",
    "timeseries_results['1d_std'] = timeseries_results['log_ret'].rolling(6*60*24).std()\n",
    "# # np.std(top_ob['log_rets'])\n",
    "# ten_s_std = np.sqrt(np.sum((timeseries_results['log_ret'] - timeseries_results['log_ret'].mean())**2)/(timeseries_results['log_ret'].shape[0]-1)) # -1 unbiased estimator\n",
    "# one_h_std = ten_s_std * np.sqrt(6*60) # assuming statistic independence of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_results['log_ret'][:300000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "            x = timeseries_results['Datetime'],\n",
    "            y = timeseries_results['1hr_std']\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "            x = timeseries_results['Datetime'],\n",
    "            y = timeseries_results['1d_std']\n",
    "    ),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "#fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.sum((timeseries_results['log_ret'] - timeseries_results['log_ret'].mean())**2)/(timeseries_results['log_ret'].shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trades_distribution(df_trades, bin_size=0.0001, metric='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trades_length_overview(df_trades, x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}