{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os, time, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# from func_tools import import_px_data, standardize, fetch_s3_trade_files, cnn_data_reshaping, reshape_lob_levels, back_to_labels, intraday_vol_ret\n",
    "import data_preprocessing as dp\n",
    "import visualization_tools as viz_t\n",
    "from labelling_class import Labels_Generator, cleaned_labels, label_insights, get_strategy_pnl\n",
    "\n",
    "import inspect\n",
    "\n",
    "import plotly_express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, BatchNormalization, LeakyReLU, concatenate, add, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from configuration import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict tf to only allocate xGB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that start date is < than end date\n",
    "# assert lob levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = timedelta(seconds=60)\n",
    "pair = 'USDT_BTC'\n",
    "date_start = '2020-11-11'\n",
    "date_end = '2021-03-31'\n",
    "lob_depth = 10\n",
    "norm_type = 'dyn_z_score'\n",
    "roll = 7200#10 mins#7200 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dyn_df, test_dyn_df, top_ob_train, top_ob_test = dp.import_px_data(frequency, pair, date_start, date_end, lob_depth, norm_type, roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dyn_df.shape, test_dyn_df.shape, top_ob_train.shape, top_ob_test.shape"
   ]
  },
  {
   "source": [
    "## Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_plot = 0\n",
    "end_plot = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mid price timeseries with clean index\n",
    "px_ts_train = top_ob_train.reset_index()['Mid_Price']\n",
    "px_ts_test = top_ob_test.reset_index()['Mid_Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series with datetime index for plotting\n",
    "mid_px_series =  pd.Series(data=px_ts_train.values, index=top_ob_train['Datetime'].values, name='Mid_Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 31\n",
    "poly = 1\n",
    "smoothed_px = Labels_Generator(mid_px_series, window, poly).get_smooth_px()\n",
    "smooth_px_series = pd.Series(data=smoothed_px.values, index=top_ob_train['Datetime'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_plot = 100000\n",
    "# end_plot = start_plot + 10000\n",
    "# viz_t.plot_timeseries(ts_list=[mid_px_series[start_plot:end_plot], smooth_px_series[start_plot:end_plot]], primary_axis=[True, True], legend=['Original price', f'Smoothed {window} {poly}'], sample_size=1, width=900, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train, smoothed_px_train, df_trades_train = cleaned_labels(px_ts_train,  sg_window=31, sg_poly=1, method='two_steps_50bps', print_details=True)\n",
    "\n",
    "# encode\n",
    "encoded_train_labels = np_utils.to_categorical(labels_train.values,3) \n",
    "\n",
    "# get transaction df\n",
    "strategy_df_train = get_strategy_pnl(px_ts_train, labels_train)\n",
    "\n",
    "viz_t.plot_labels_line(px_ts_train[start_plot:end_plot], \n",
    "    labels_train[start_plot:end_plot], \n",
    "    title='Train Labels', \n",
    "    smoothed_signal=smoothed_px_train[start_plot:end_plot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test, smoothed_px_test, df_trades_test = cleaned_labels(px_ts_test, sg_window=31, sg_poly=1, method='two_steps_50bps', print_details=True)\n",
    "\n",
    "# encode\n",
    "encoded_test_labels = np_utils.to_categorical(labels_test.values,3) \n",
    "\n",
    "# get transaction df\n",
    "strategy_df_test = get_strategy_pnl(px_ts_test, labels_test)\n",
    "\n",
    "viz_t.plot_labels_line(px_ts_test[start_plot:end_plot], \n",
    "    labels_test[start_plot:end_plot], \n",
    "    title='Test Labels', \n",
    "    smoothed_signal=smoothed_px_test[start_plot:end_plot])"
   ]
  },
  {
   "source": [
    "## Visual checks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_trades_distribution(df_trades_train[df_trades_train['cleaned_labels']!=0], bin_size=0.0001, metric='gross_returns', fig_width=900, fig_height=550)\n",
    "\n",
    "viz_t.plot_trades_length_overview(df_trades_train[df_trades_train['cleaned_labels']!=0], x='trade_len',  y='gross_returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram()\n",
    "fig.add_trace(go.Histogram(x=df_trades_train['trade_len'].values, name='train', autobinx = False, xbins={'size':5}))\n",
    "fig.add_trace(go.Histogram(x=df_trades_test['trade_len'].values, name='test', autobinx = False, xbins={'size':5}))\n",
    "\n",
    "# The two histograms are drawn on top of another\n",
    "fig.update_layout(barmode='overlay')\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data - is it needed?\n",
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = dp.reshape_lob_levels(train_dyn_df.reset_index(), output_type='array') # 1 train dataset\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2, index=train_dt_index_dyn) # 2\n",
    "px_ts_train = top_ob_train.reset_index()[['Mid_Price']]\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = dp.reshape_lob_levels(test_dyn_df.reset_index(), output_type='array') # 1 test dataset\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2, index=test_dt_index_dyn) # 2\n",
    "px_ts_test = top_ob_test.reset_index()[['Mid_Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_t.plot_timeseries(ts_list=[top_ob_train.set_index('Datetime')['Mid_Price'], top_ob_test.set_index('Datetime')['Mid_Price'], mid_px_train_dyn, mid_px_test_dyn], primary_axis=[True, True, False, False], legend=['train-px', 'test-px', 'train-dyn', 'test-dyn'], sample_size=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dyn_df[train_dyn_df['Level']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trades\n",
    "raw_file_path = './Experiments/input/raw/trades/USDT_BTC/USDT_BTC-20210330.csv.gz'\n",
    "day_data = pd.read_csv(raw_file_path, parse_dates=['date'])\n",
    "day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process trades\n",
    "df_trades_grp = day_data.groupby([pd.Grouper(key='date', freq='1min'), 'type']).agg({'amount':np.sum, 'rate':np.mean, 'orderNumber':pd.Series.nunique,  'globalTradeID':'count'})\n",
    "\n",
    "\n",
    "wtavg = lambda x: np.average(x['rate'], weights=x['amount'], axis=0)\n",
    "dfwavg = day_data.groupby([pd.Grouper(key='date', freq='1min'), 'type']).apply(wtavg)\n",
    "dfwavg.name = 'wav_price'\n",
    "\n",
    "df_trades_grp = pd.merge(df_trades_grp, dfwavg, left_index=True, right_index=True).reset_index()\n",
    "df_trades_grp.rename(columns={'date':'Datetime', 'rate':'av_price', 'orderNumber':'unique_orders', 'globalTradeID':'clips'}, inplace=True)\n",
    "\n",
    "df_trades_piv = df_trades_grp.pivot(values=['amount', 'av_price', 'wav_price', 'unique_orders', 'clips'], columns='type',index='Datetime').reset_index()\n",
    "\n",
    "df_trades_piv.columns = list(map(\"_\".join, df_trades_piv.columns)) # \"flatten\" column names\n",
    "\n",
    "df_trades_piv.rename(columns={'Datetime_':'Datetime'}, inplace=True)\n",
    "df_trades_piv.set_index('Datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# import lob\n",
    "# Load all files in to a dictionary\n",
    "configuration = config()\n",
    "raw_data_folder = configuration['folders']['raw_lob_data']\n",
    "\n",
    "pair = 'USDT_BTC'\n",
    "date_to_process = datetime.strptime('2021-03-30', '%Y-%m-%d')\n",
    "day_folder = datetime.strftime(date_to_process, '%Y/%m/%d')\n",
    "lob_depth = 100\n",
    "\n",
    "raw_data = {} # empty dict to update with incoming json\n",
    "for file_name in os.listdir(f'{raw_data_folder}/{pair}/{day_folder}'):\n",
    "\n",
    "    try:\n",
    "        with gzip.open(f'{raw_data_folder}/{pair}/{day_folder}/{file_name}', 'r') as f:\n",
    "            json_string = f.read().decode('utf-8')\n",
    "            frozen = json_string.count('\"isFrozen\": \"1\"')\n",
    "            if frozen > 0:\n",
    "                print(f'Frozen {frozen} snapshots')\n",
    "        raw_data_temp = dp.load_lob_json(json_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e.errno)\n",
    "        print(e)\n",
    "\n",
    "    raw_data.update(raw_data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_data = []\n",
    "raw_data_frame = pd.DataFrame.from_dict(raw_data, orient='index')\n",
    "raw_data_frame.reset_index(inplace=True)\n",
    "raw_data_frame['index'] = raw_data_frame['index'].str[-15:]\n",
    "raw_data_frame['index'] = pd.to_datetime(raw_data_frame['index'], format='%Y%m%d_%H%M%S')\n",
    "raw_data_frame.set_index('index',drop=True,inplace=True)\n",
    "raw_data_frame.sort_index(inplace=True)\n",
    "idx_start = date_to_process\n",
    "idx_end = date_to_process + timedelta(days=1) - timedelta(seconds=1)\n",
    "idx = pd.date_range(idx_start, idx_end, freq='1s')\n",
    "raw_data_frame = raw_data_frame.reindex(idx).ffill().fillna(method='bfill') # forward fill gaps and back fill first item if missing\n",
    "\n",
    "# Convert hierarchical json data in to tabular format\n",
    "levels = list(range(lob_depth))\n",
    "for row in raw_data_frame.itertuples():\n",
    "\n",
    "    ask_price, ask_volume = zip(* row.asks[0:lob_depth])\n",
    "    bid_price, bid_volume = zip(* row.bids[0:lob_depth])\n",
    "    sequences = [row.seq] * lob_depth\n",
    "    datetimes = [row.Index] * lob_depth\n",
    "\n",
    "    processed_data.append(list(zip(\n",
    "        ask_price,\n",
    "        ask_volume,\n",
    "        bid_price,\n",
    "        bid_volume,\n",
    "        levels,\n",
    "        sequences,\n",
    "        datetimes\n",
    "    )))\n",
    "\n",
    "# unravel nested structure and force data types\n",
    "day_data = pd.DataFrame([y for x in processed_data for y in x], #flatten the list of lists structure\n",
    "                columns = ['Ask_Price', 'Ask_Size', 'Bid_Price', 'Bid_Size','Level', 'Sequence','Datetime'])\n",
    "\n",
    "day_data['Ask_Price'] = day_data['Ask_Price'].astype('float64')\n",
    "day_data['Bid_Price'] = day_data['Bid_Price'].astype('float64')\n",
    "day_data['Sequence'] = day_data['Sequence'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "day_data.sort_values(by=['Datetime', 'Level'], inplace=True)\n",
    "day_data['Mid_Price'] = (day_data['Ask_Price'] + day_data['Bid_Price'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data['Prevailing_Mid'] = day_data.groupby('Datetime')['Mid_Price'].transform('first')\n",
    "day_data['Bid_Cum_Size'] = day_data.groupby('Datetime')['Bid_Size'].transform(np.cumsum)\n",
    "day_data['Ask_Cum_Size'] = day_data.groupby('Datetime')['Ask_Size'].transform(np.cumsum)\n",
    "\n",
    "day_data['Bid_Spread'] = (day_data['Bid_Price'] - day_data['Prevailing_Mid']) / day_data['Prevailing_Mid'] * -1\n",
    "day_data['Ask_Spread'] = (day_data['Ask_Price'] - day_data['Prevailing_Mid']) / day_data['Prevailing_Mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data.groupby([pd.Grouper(key='Datetime', freq='1min'), 'Level']).agg({\n",
    "    'Bid_Spread':np.mean, \n",
    "    'Ask_Spread':np.mean, \n",
    "    'Bid_Cum_Size':np.mean, \n",
    "    'Ask_Cum_Size':np.mean, \n",
    "    'Bid_Price':np.mean, \n",
    "    'Ask_Price':np.mean,\n",
    "    'Bid_Size':np.mean, \n",
    "    'Ask_Size':np.mean\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_data.head(103)#['Bid_Spread'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot1 = day_data[day_data['Level']==0].copy()\n",
    "df_plot1.set_index('Datetime', inplace=True)\n",
    "df_plot100 = day_data[day_data['Level']==10].copy()\n",
    "df_plot100.set_index('Datetime', inplace=True)\n",
    "fig = px.line(render_mode='svg')\n",
    "fig.add_scatter(y=df_plot1['Ask_Price'], x=df_plot1.index, name='ask')\n",
    "fig.add_scatter(y=df_plot100['Ask_Price'], x=df_plot100.index, name='ask')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(day_data.set_index('Datetime')['Ask_Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime('2021-03-30', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_trades_piv['wav_price_buy'], render_mode='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_trades_piv['av_price_buy'], render_mode='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades_piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_trades = dp.get_trade_data(pair, date_start, date_end, frequency=frequency)\n",
    "processed_trades = dask_trades.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_trades['Datetime'] = pd.to_datetime(processed_trades['Datetime'])\n",
    "processed_trades.set_index('Datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noticed that before we were taking last of the aggregation\n",
    "# savitzky golay to trades\n",
    "# savitzky golay to training set, one filter per time series\n",
    "# size weighted average price when grouping is better than simple average\n",
    "# add number of trades as well\n",
    "# add trade spread from prevailing mid?\n",
    "\n",
    "\n",
    "# volume weighted average price for trades\n",
    "# no savitki filter, info leakage\n",
    "# no order book levels, but feed aggregated spread weighted, like +/- bps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_plot = 100000\n",
    "end_plot = start_plot + 10000\n",
    "trades_ask_size = processed_trades.sort_values('Datetime')['Bid_Size'].rolling(5).mean()\n",
    "viz_t.plot_timeseries(ts_list=[mid_px_series[start_plot:end_plot], smooth_px_series[start_plot:end_plot], trades_ask_size[start_plot+7200:end_plot+7200]], primary_axis=[True, True, False], legend=['Original price', f'Smoothed {window} {poly}', 'Trades ask size'], sample_size=1, width=900, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(processed_trades.sort_values('Datetime')['Ask_Size'].rolling(60).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_trades.sort_values('Datetime')['Ask_Price'][:140000].rolling(60).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_trades.sort_values('Datetime')['Bid_Size'][:140000].rolling(60).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: 'amount_buy':'Ask_Size', 'amount_sell':'Bid_Size', 'rate_buy':'Ask_Price', 'rate_sell':'Bid_Price'\n",
    "df_trading_activity = train_dyn_df[train_dyn_df['Level']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(y=df_trading_activity['Ask_Size'].values, x=df_trading_activity['Datetime'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trading_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_depth_dyn.shape"
   ]
  },
  {
   "source": [
    "## Model Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_light_deeplob(T, lob_depth):\n",
    "    ## just a test\n",
    "\n",
    "    input_lmd = Input(shape=(T, lob_depth * 4, 1))\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)    \n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    # conv_first1 = Dropout(.5)(conv_first1)\n",
    "    \n",
    "    # note on learnable parameters: CONV2(filter shape =1*2, stride=1) layer is: ((shape of width of filter * shape of height filter * number of filters in the previous layer+1) * number of filters) = 2080 or ((2*1*32)+1)*32\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(16, (1, lob_depth))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = BatchNormalization()(conv_first1)\n",
    "    print(conv_first1.shape)\n",
    "\n",
    "    convfirst_output = Reshape((int(conv_first1.shape[1])* int(conv_first1.shape[3]),))(conv_first1)\n",
    "    print(convfirst_output.shape)\n",
    "    # note on learnable parameters:FC3 layer is((current layer c*previous layer p)+1*c) with c being number of neurons\n",
    "    out = Dense(3, activation='softmax')(convfirst_output)\n",
    "    print(out.shape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_code = inspect.getsource(create_light_deeplob)\n",
    "lines_with_short_desription = [line for line in model_code.split('\\n') if \"##\" in line]\n",
    "short_description = re.sub(r'\\W+', '_', lines_with_short_desription[0])\n",
    "\n",
    "create_light_deeplob(length, lob_depth).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_now = datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "experiment_id = f'{date_time_now}-{pair}-{frequency.seconds}s-{lob_depth}l-{length}-{date_start}-{date_end}{short_description}'\n",
    "results_folder = f'{experiments_folder}/{pair}/{experiment_id}'\n",
    "os.makedirs(f'{results_folder}', exist_ok=True)\n",
    "batch_size=256\n",
    "\n",
    "config = {\n",
    "  'pair': pair,\n",
    "  'frequency': frequency.seconds,\n",
    "  'lob_depth': lob_depth,\n",
    "  'length': length,\n",
    "  'date_start': date_start,\n",
    "  'date_end': date_end,\n",
    "  'norm_type': norm_type,\n",
    "  'roll': roll,\n",
    "  'batch_size': batch_size,\n",
    "  'label_technique': label_technique\n",
    "#   'min_profit': min_profit,\n",
    "#   'k_plus': k_plus,\n",
    "#   'k_minus': k_minus,\n",
    "#   'alpha': alpha,\n",
    "#   'trading_fee': trading_fee,\n",
    "\n",
    "#   'input': input_file_name,\n",
    "#   'normalized_train_file': normalized_train_file,\n",
    "#   'normalized_test_file':   normalized_test_file,\n",
    "#   'top_ob_train_file': top_ob_train_file,\n",
    "#   'top_ob_test_file': top_ob_test_file\n",
    "}\n",
    "\n",
    "with open(f'{results_folder}/config.json', 'w') as fp:\n",
    "    json.dump(config, fp, default=str)\n",
    "\n",
    "with open(f'{results_folder}/model_code.py', 'w') as fp:\n",
    "    fp.write(model_code)\n",
    "\n",
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "with open(f'{results_folder}/model_summary.txt', 'w') as fp:\n",
    "    light_deeplob.summary(print_fn=lambda x: fp.write(x + '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to train the model on smoother version of the data"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_deeplob = create_light_deeplob(length, lob_depth)\n",
    "\n",
    "model_checkpoint_path = f'{results_folder}/{experiment_id}.h5'\n",
    "\n",
    "# Learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                   factor=0.2, \n",
    "                                                   patience=20)\n",
    "\n",
    "# Checkpoint callback. Saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=2,\n",
    "                                                 save_freq='epoch') # every epoch\n",
    "\n",
    "# Early stopping callback. When sees no progress on the validation set\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "# Tensorboard callback\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(results_folder)\n",
    "\n",
    "# Train and Test time series generators\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth_dyn,\n",
    "    encoded_train_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# to be replaced with validation?\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_/210119-184504-USDT_BTC-10s-10l-300-2020_04_04-2021_01_03_binary_classification_full_inception_lighter_deep_lob_model_with_longer_timesteps_300_.h5'\n",
    "# loaded_light_deep_lob = tf.keras.models.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "light_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=0,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/home/federico/Python_vsc_dir/RL_Trader/Experiments/USDT_BTC/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle/210221-200759-USDT_BTC-10s-10l-100-2020_04_04-2021_01_03_big_lr_big_batch_size_16_filter_size_shuffle.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved weights and evaluate model performance\n",
    "deep_lob_loaded = tf.keras.models.load_model(model_name)\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    length,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "def evaluate_model(model):\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(generator_test, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "\n",
    "#evaluate_model(deep_lob_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels\n",
    "predictions_prob = deep_lob_loaded.predict(generator_test, verbose=1)\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels = pd.Series(map_labels(np.argmax(predictions_prob,axis=1)), name='predicted_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental: predicted labels on rolling avg\n",
    "predictions_prob_wa = pd.DataFrame(predictions_prob).rolling(window=10).mean().values\n",
    "map_labels = np.vectorize(back_to_labels) # vectorize back to labels from func_tools\n",
    "predicted_labels_wa = pd.Series(map_labels(np.argmax(predictions_prob_wa,axis=1)), name='predicted_labels_wa') # back to original 1,0,-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Predicted labels #####')\n",
    "label_insights(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('##### Weighted average predicted labels #####')\n",
    "label_insights(predicted_labels_wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels.shape, test_depth_dyn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dangerous assigning offset here, wrap it into a function\n",
    "# align prediction with \"length\" offset\n",
    "index_range = np.arange(length, predicted_labels.shape[0] + length) # offset ts length fed to ts generator\n",
    "predicted_labels.index = index_range\n",
    "\n",
    "# generate timeseries with buy, sell, zero prob\n",
    "buy_prob = pd.Series(predictions_prob[:,1], index=index_range)\n",
    "sell_prob = pd.Series(predictions_prob[:,2], index=index_range)\n",
    "zero_prob = pd.Series(predictions_prob[:,0], index=index_range)\n",
    "\n",
    "buy_prob_wa = pd.Series(predictions_prob_wa[:,1], index=index_range)\n",
    "\n",
    "viz_t.plot_labels_line(top_ob_test['Mid_Price'][start_plot:end_plot], \n",
    "    labels_test[start_plot:end_plot], # original labels\n",
    "    title='Train Set Labels', \n",
    "    #smoothed_signal=smoothed_px_test[start_plot:end_plot],\n",
    "    predicted_labels=predicted_labels[start_plot:end_plot],\n",
    "    buy_prob_labels=buy_prob[start_plot:end_plot],\n",
    "    #sell_prob_labels=sell_prob[start:end],\n",
    "    predictions_prob_wa=buy_prob_wa[start_plot:end_plot],\n",
    "    width=1100, height=600\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOUBLE CHECK that labels and px_ts are correctly aligned\n",
    "px_ts = top_ob_test['Mid_Price']# adjust prediction offsset\n",
    "datetime_ts = top_ob_test['Datetime']\n",
    "trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# px_ts = top_ob_test['Mid_Price'][length:].reset_index()['Mid_Price']# adjust prediction offsset\n",
    "# datetime_ts = top_ob_test['Datetime'][length:].reset_index()['Datetime']\n",
    "# trades_timeseries = get_strategy_pnl(px_ts, predicted_labels)\n",
    "# df_trades = trades_timeseries.dropna(subset=['gross_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to do:\n",
    "# seek for patterns in prediction probability\n",
    "# day vs night - weekday vs weekend - model certainty before long trades vs short trades\n",
    "# determine if predictions are naive"
   ]
  }
 ]
}