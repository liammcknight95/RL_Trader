{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly_express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "# for device in tensorflow.config.experimental.list_physical_devices('GPU'):\n",
    "#     tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, LSTM, Dense, MaxPooling2D, LeakyReLU, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "import LOBData\n",
    "from func_tools import normalize, get_labels, cnn_data_reshaping, reshape_lob_levels, plot_labels, label_insights, get_pnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict tf to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "source": [
    "## Inputs and data import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing inputs\n",
    "security = 'USDT_BTC'\n",
    "raw_data_path = f'S3_data' # where json data is stored\n",
    "root_caching_folder = \"Processed_Data\"\n",
    "frequency = timedelta(seconds=10)\n",
    "norm_type = 'dyn_z_score'\n",
    "\n",
    "# labelling inputs\n",
    "k_plus = 30#60\n",
    "k_minus = 30#60\n",
    "alpha = 0.001#0.0005\n",
    "roll = 7200 * 6 # step from minute to 10 second data\n",
    "# pull data from S3\n",
    "#download_s3_data('limit-order-books-data-po-limitorderbooksnapshots-v25ungbmmak9', pair)\n",
    "\n",
    "# Data import - needs to be adjusted importing from several files using Dask\n",
    "data = pd.read_csv(f'{root_caching_folder}/{security}/data-cache-10s.csv', index_col=0)\n",
    "lob_depth = data['Level'].max() + 1 # number of levels of order book"
   ]
  },
  {
   "source": [
    "## Data preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_test_split = int((data.shape[0] / lob_depth) * 0.7) # slice reference for train and test\n",
    "train_timestamps = data['Datetime'].unique()[:train_test_split]\n",
    "test_timestamps = data['Datetime'].unique()[train_test_split:]\n",
    "\n",
    "train_cached_data = data[data['Datetime'].isin(train_timestamps)].set_index(['Datetime', 'Level'])\n",
    "test_cached_data = data[data['Datetime'].isin(test_timestamps)].set_index(['Datetime', 'Level'])\n",
    "\n",
    "print(f'Train dataset shape: {train_cached_data.shape} - Test dataset shape: {test_cached_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized data size & price standardization for train and test set\n",
    "number_of_workers = 4\n",
    "inputs = (\n",
    "    (train_cached_data[['Ask_Price', 'Bid_Price']], lob_depth, 'dyn_z_score', roll),\n",
    "    (train_cached_data[['Ask_Size', 'Bid_Size']], lob_depth, 'dyn_z_score', roll),\n",
    "    (test_cached_data[['Ask_Price', 'Bid_Price']], lob_depth, 'dyn_z_score', roll),\n",
    "    (test_cached_data[['Ask_Size', 'Bid_Size']], lob_depth, 'dyn_z_score', roll)\n",
    "    )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with multiprocessing.Pool(number_of_workers) as p:\n",
    "    res = p.starmap(normalize, inputs)\n",
    "    res = list(res)\n",
    "    #print(res)\n",
    "    p.close()   \n",
    "    p.join()\n",
    "    \n",
    "train_dyn_prices, train_dyn_volumes, test_dyn_prices, test_dyn_volumes = res[0], res[1], res[2], res[3]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# concat prices and volumes back together and top level (useful for Dash)\n",
    "train_dyn_df = pd.concat([train_dyn_prices, train_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "train_dyn_df[train_dyn_df['Level']==0].to_csv(f'{root_caching_folder}/{security}/TRAIN-{lob_depth}-{norm_type}-{roll}.csv') # save top level to csv \n",
    "\n",
    "test_dyn_df = pd.concat([test_dyn_prices, test_dyn_volumes], axis=1).reset_index() # concat along row index\n",
    "test_dyn_df[test_dyn_df['Level']==0].to_csv(f'{root_caching_folder}/{security}/TEST-{lob_depth}-{norm_type}-{roll}.csv') # save top level to csv \n",
    "\n",
    "display(train_dyn_df.describe()) # check train data overview\n",
    "display(test_dyn_df.describe()) # check test data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 reshape to a format suitable for training\n",
    "# 2 get mid px from normalized data\n",
    "# 3 get labels from norm mid prices\n",
    "# 4 labels one hot encoding\n",
    "\n",
    "# train\n",
    "train_depth_dyn, train_dt_index_dyn = reshape_lob_levels(train_dyn_df, output_type='array') # 1 train dataset\n",
    "mid_px_train_dyn = pd.Series((train_depth_dyn[:,2] + train_depth_dyn[:,0]) / 2) # 2\n",
    "labels_dyn_train = get_labels(mid_px_train_dyn, k_plus, k_minus, alpha, long_only=False) # 3\n",
    "encoded_train_labels = np_utils.to_categorical(labels_dyn_train.values,3) # 4 train labels\n",
    "\n",
    "# test\n",
    "test_depth_dyn, test_dt_index_dyn = reshape_lob_levels(test_dyn_df, output_type='array') # 1 test dataset\n",
    "mid_px_test_dyn = pd.Series((test_depth_dyn[:,2] + test_depth_dyn[:,0]) / 2) # 2\n",
    "labels_dyn_test = get_labels(mid_px_test_dyn, k_plus, k_minus, alpha, long_only=False) # 3\n",
    "encoded_test_labels = np_utils.to_categorical(labels_dyn_test.values,3) # 4 test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the newly generated labels\n",
    "print('Train Labels')\n",
    "train_transact_dyn = label_insights(labels_dyn_train)\n",
    "print('\\nTest Labels')\n",
    "test_transact_dyn = label_insights(labels_dyn_test)\n",
    "print(f'\\nLabels Train as pctg of total: {test_transact_dyn/(test_transact_dyn+train_transact_dyn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels sanity check\n",
    "def plot_data(norm_mid_px, labels, start, end, train_test_switch, y0=0):\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=1,specs=[[{\"secondary_y\": True}]])\n",
    "    fig.update_layout(title=f'<b>Visual check: {train_test_switch}</b>', title_x=0.5)\n",
    "\n",
    "    fig.add_trace(go.Scatter(y=norm_mid_px.values[start:end], x=norm_mid_px.index[start:end], name='mix_px_dyn_train'))   \n",
    "    #fig.add_trace(go.Scatter(y=labels_dyn_train[start:end], name='labels_encoded'), secondary_y=True)\n",
    "\n",
    "    background_color = plot_labels(labels[start:end], y0) # funct_tools formula to plot labels\n",
    "    \n",
    "    fig.update_layout(width=1200, \n",
    "        height=600,\n",
    "        shapes=background_color,\n",
    "        xaxis2= {'anchor': 'x','overlaying': 'x', 'side': 'top'},\n",
    "        yaxis_domain=[0, 1])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train data\n",
    "plot_data(mid_px_train_dyn, labels_dyn_train, 0, 30000, train_test_switch='train',y0=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test data\n",
    "plot_data(mid_px_test_dyn, labels_dyn_test, 0, 10000, train_test_switch='test',y0=0)"
   ]
  },
  {
   "source": [
    "## Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Modified deeplob\n",
    "def create_light_deeplob(T, NF, number_of_lstm):\n",
    "    \n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(16, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(16, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(64, (1, 10))(conv_first1)\n",
    "    conv_first1 = LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    \n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "    \n",
    "    convsecond_output = concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    print(convsecond_output.shape)\n",
    "    # use the MC dropout here\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]) * int(convsecond_output.shape[3]),))(convsecond_output)\n",
    "\n",
    "    conv_reshape = Dropout(rate=0.2)(conv_reshape)\n",
    "    out = Dense(3, activation='softmax')(conv_reshape)\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "ligh_deeplob = create_light_deeplob(100, 40, 64)\n",
    "ligh_deeplob.summary()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare path to store tensorboard logs\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "model_name = \"dynz_score_lob_v3_10s.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate callback. Reduce on Plateau multiply the lr by the factor if val loss does not improve for n epochs (patience)\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                   factor=0.5, \n",
    "                                                   patience=5)\n",
    "\n",
    "# Checkpoint callback. Saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(model_name,\n",
    "                                                 save_best_only=True,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1,\n",
    "                                                 period=1) # every epoch\n",
    "\n",
    "# Early stopping callback. When sees no progress on the validation set\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "# Tensorboard callback\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# Train and Test time series generators\n",
    "generator_train = TimeseriesGenerator(\n",
    "    train_depth_dyn,\n",
    "    encoded_train_labels,\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# to be replaced with validation?\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "ligh_deeplob.fit(generator_train, \n",
    "            epochs=200, \n",
    "            verbose=1,\n",
    "            validation_data=generator_test,\n",
    "            callbacks=[lr_callback, cp_callback, es_callback, tb_callback])\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "source": [
    "## Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predictions\n",
    "\n",
    "# Load the previously saved weights\n",
    "deep_lob_loaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "generator_test = TimeseriesGenerator(\n",
    "    test_depth_dyn,\n",
    "    encoded_test_labels,\n",
    "    100,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = deep_lob_loaded.evaluate(generator_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse engineer how to_categorical have assigned labels and map them back\n",
    "\n",
    "def back_to_labels(x):\n",
    "\n",
    "    if x == 0:\n",
    "        return 0\n",
    "\n",
    "    elif x == 1:\n",
    "        return 1\n",
    "\n",
    "    elif x == 2:\n",
    "        return -1\n",
    "        \n",
    "predictions = deep_lob_loaded.predict(generator_test, verbose=1)\n",
    "#print(np.hstack([np.where(predictions==0)[0][0], np.where(predictions==1)[0][0], np.where(predictions==-1)[0][0]]))# first element\n",
    "\n",
    "map_labels = np.vectorize(back_to_labels)\n",
    "mapped_labels = map_labels(np.argmax(predictions,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_pctg_df = pd.DataFrame(predictions, columns=[0, 1, -1]) # output pctg at each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(mid_px_test_dyn[100:].reset_index()[0], pd.Series(mapped_labels), 0, 30100, train_test_switch='predictions',y0=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted labels, real labels, difference between the two and probability of a \"buy\" label\n",
    "fig = make_subplots(rows=1, cols=1,specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.update_layout(title=f'<b>Label Comparison</b>', title_x=0.5)\n",
    "\n",
    "fig.add_trace(go.Scatter(y=mapped_labels[0:3000]+5, name='predicted'))\n",
    "fig.add_trace(go.Scatter(y=labels_dyn_test[100:][0:3000].values-5, name='labels'))\n",
    "fig.add_trace(go.Scatter(y=mapped_labels[0:3000] - labels_dyn_test[100:][0:3000].values, name='predictions - labels'))\n",
    "\n",
    "fig.add_trace(go.Scatter(y=predictions_pctg_df[1][0:3000], name = 'Probability of 1'), secondary_y=True)\n",
    "\n",
    "fig.update_layout(width=1200, height=600) # plot labels background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}